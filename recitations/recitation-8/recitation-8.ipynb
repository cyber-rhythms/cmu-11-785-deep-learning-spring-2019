{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of a encoder & decoder network. The purpose of this code is to show a typical encoder-decoder\n",
    "#  structure. I hide/modify the attention part since students will need to implement it in the hw4. \n",
    "\n",
    "# Do not copy the attention part of this code, it's not the right version for hw.\n",
    "# Do not copy the attention part of this code, it's not the right version for hw.\n",
    "# Do not copy the attention part of this code, it's not the right version for hw.\n",
    "\n",
    "\n",
    "class MynetWork(nn.Module):\n",
    "    def __init__(self, src_vocab, embedding_size, output_size, target_vocav):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, embedding_size)\n",
    "        self.decoder = Decoder(trg_vocab, output_size)\n",
    "        self.out = nn.Linear(output_size, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, target_vocav):\n",
    "        e_outputs = self.encoder(src)\n",
    "        d_output = self.decoder(e_outputs)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "\n",
    "class Eecoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Listener, self).__init__()\n",
    "\n",
    "        self.pblstm1 = pBLSTM(input_dim, hidden_dim)\n",
    "        self.pblstm2 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "        self.pblstm3 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.mlp1 = MLP(hidden_dim*2, 256, 128)\n",
    "        self.mlp2 = MLP(hidden_dim*2, 256, 128)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pblstm1(x)\n",
    "        x = self.pblstm2(x)\n",
    "        x = self.pblstm3(x)\n",
    "        x = self.dropout2(x)  \n",
    "        \n",
    "        keys = self.mlp1(x)\n",
    "        values = self.mlp2(x)       \n",
    "        return keys, values\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, vocab_size):\n",
    "        super(Listener, self).__init__()\n",
    "\n",
    "        self.pblstm1 = pBLSTM(input_dim, hidden_dim)\n",
    "        self.pblstm2 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "        self.pblstm3 = pBLSTM(hidden_dim*2, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.mlp1 = MLP(hidden_dim*2, 256, 128)\n",
    "        self.myAttention = Attention()\n",
    "\n",
    "    def forward(self, keys, values, y):\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        context = Attention(keys, values)\n",
    "        \n",
    "        # y will the target seuquence \n",
    "        embedding_y = self.embedding(y)\n",
    "        \n",
    "        for i in range(y.shape[1] - 1):\n",
    "            embedding_yi = embedding_y[:,i,:]\n",
    "            x = torch.cat((embedding_yi, context), dim=1)\n",
    "            \n",
    "            # x = embedding_yi + context\n",
    "            x = self.pblstm1(x)\n",
    "            x = self.pblstm2(x)\n",
    "            x = self.pblstm3(x)\n",
    "            x = self.dropout2(x)\n",
    "            output = self.mlp1(x)\n",
    "            result.append(output) \n",
    "            \n",
    "        result = np.asarray(result)\n",
    "        return result\n",
    "\n",
    "\n",
    "class pBLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.blstm = nn.LSTM(input_dim*2, hidden_dim, 1, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # reduce the timestep\n",
    "        x = x.contiguous().view(batch_size, int(seq_length//2), feature_dim*2)\n",
    "        output, _ = self.blstm(x)\n",
    "        return output\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
