{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.rnn as rnn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import shakespeare_data as sh\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed length input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 203 characters...Last 50 characters\n",
      "1609\n",
      " THE SONNETS\n",
      " by William Shakespeare\n",
      "                      1\n",
      "   From fairest creatures we desire increase,\n",
      "   That thereby beauty's rose might never die,\n",
      "   But as the riper should by time decease,\n",
      "...,\n",
      "   And new pervert a reconciled maid.'\n",
      " THE END\n",
      "\n",
      "Total character count: 5551930\n",
      "Unique character count: 84\n",
      "\n",
      "shakespeare_array.shape: (5551930,)\n",
      "\n",
      "First 17 characters as indices [12 17 11 20  0  1 45 33 30  1 44 40 39 39 30 45 44]\n",
      "First 17 characters as characters: ['1', '6', '0', '9', '\\n', ' ', 'T', 'H', 'E', ' ', 'S', 'O', 'N', 'N', 'E', 'T', 'S']\n",
      "First 17 character indices as text:\n",
      " 1609\n",
      " THE SONNETS\n"
     ]
    }
   ],
   "source": [
    "# Data - refer to shakespeare_data.py for details\n",
    "corpus = sh.read_corpus()\n",
    "print(\"First 203 characters...Last 50 characters\")\n",
    "print(\"{}...{}\".format(corpus[:203], corpus[-50:]))\n",
    "print(\"Total character count: {}\".format(len(corpus)))\n",
    "chars, charmap = sh.get_charmap(corpus)\n",
    "charcount = len(chars)\n",
    "print(\"Unique character count: {}\\n\".format(len(chars)))\n",
    "shakespeare_array = sh.map_corpus(corpus, charmap)\n",
    "print(\"shakespeare_array.shape: {}\\n\".format(shakespeare_array.shape))\n",
    "small_example = shakespeare_array[:17]\n",
    "print(\"First 17 characters as indices\", small_example)\n",
    "print(\"First 17 characters as characters:\", [chars[c] for c in small_example])\n",
    "print(\"First 17 character indices as text:\\n\", sh.to_text(small_example,chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class. Transform raw text into a set of sequences of fixed length, and extracts inputs and targets\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,text, seq_len = 200):\n",
    "        n_seq = len(text) // seq_len\n",
    "        text = text[:n_seq * seq_len]\n",
    "        self.data = torch.tensor(text).view(-1,seq_len)\n",
    "    def __getitem__(self,i):\n",
    "        txt = self.data[i]\n",
    "        return txt[:-1],txt[1:]\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "# Collate function. Transform a list of sequences into a batch. Passed as an argument to the DataLoader.\n",
    "# Returns data on the format seq_len x batch_size\n",
    "def collate(seq_list):\n",
    "    inputs = torch.cat([s[0].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    targets = torch.cat([s[1].unsqueeze(1) for s in seq_list],dim=1)\n",
    "    return inputs,targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class CharLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,embed_size,hidden_size, nlayers):\n",
    "        super(CharLanguageModel,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers) # Recurrent network\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size) # Projection layer\n",
    "        \n",
    "    def forward(self,seq_batch): #L x N\n",
    "        # returns 3D logits\n",
    "        batch_size = seq_batch.size(1)\n",
    "        embed = self.embedding(seq_batch) #L x N x E\n",
    "        hidden = None\n",
    "        output_lstm,hidden = self.rnn(embed,hidden) #L x N x H\n",
    "        output_lstm_flatten = output_lstm.view(-1,self.hidden_size) #(L*N) x H\n",
    "        output_flatten = self.scoring(output_lstm_flatten) #(L*N) x V\n",
    "        return output_flatten.view(-1,batch_size,self.vocab_size)\n",
    "    \n",
    "    def generate(self,seq, n_words): # L x V\n",
    "        # performs greedy search to extract and return words (one sequence).\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "        return torch.cat(generated_words,dim=0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    before = time.time()\n",
    "    print(\"training\", len(train_loader), \"number of batches\")\n",
    "    for batch_idx, (inputs,targets) in enumerate(train_loader):\n",
    "        if batch_idx == 0:\n",
    "            first_time = time.time()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        outputs = model(inputs) # 3D\n",
    "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1)) # Loss of the flattened outputs\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\"Time elapsed\", time.time() - first_time)\n",
    "            \n",
    "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "            after = time.time()\n",
    "            print(\"Time: \", after - before)\n",
    "            print(\"Loss per word: \", loss.item() / batch_idx)\n",
    "            print(\"Perplexity: \", np.exp(loss.item() / batch_idx))\n",
    "            after = before\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    for inputs,targets in val_loader:\n",
    "        batch_id+=1\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1,outputs.size(2)),targets.view(-1))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / batch_id\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharLanguageModel(charcount,256,256,3)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "split = 5000000\n",
    "train_dataset = TextDataset(shakespeare_array[:split])\n",
    "val_dataset = TextDataset(shakespeare_array[split:])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = collate)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 391 number of batches\n",
      "Time elapsed 0.14118456840515137\n",
      "Time:  5.480123996734619\n",
      "Loss per word:  0.06285614013671875\n",
      "Perplexity:  1.0648736356360111\n",
      "\n",
      "Validation loss per word: 1.7341472437215406\n",
      "Validation perplexity : 5.664095650645451 \n",
      "\n",
      "training 391 number of batches\n",
      "Time elapsed 0.10048866271972656\n",
      "Time:  5.435199022293091\n",
      "Loss per word:  0.03288536071777344\n",
      "Perplexity:  1.0334320605406284\n",
      "\n",
      "Validation loss per word: 1.5287958910298902\n",
      "Validation perplexity : 4.612619380716595 \n",
      "\n",
      "training 391 number of batches\n",
      "Time elapsed 0.10138869285583496\n",
      "Time:  5.433004140853882\n",
      "Loss per word:  0.0290824031829834\n",
      "Perplexity:  1.029509425833496\n",
      "\n",
      "Validation loss per word: 1.4559050049892692\n",
      "Validation perplexity : 4.288362699613226 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    train_epoch(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, seed,nwords):\n",
    "    seq = sh.map_corpus(seed, charmap)\n",
    "    seq = torch.tensor(seq).to(DEVICE)\n",
    "    out = model.generate(seq,nwords)\n",
    "    return sh.to_text(out.cpu().detach().numpy(),chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uiet of \n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"To be, or not to be, that is the q\",8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the world to the world to the world\n",
      "     The world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to the world to the world to the world\n",
      "     That the world to th\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Richard \", 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1609\n",
      "\n",
      " THE SONNETS\n",
      "\n",
      " by William Shakespeare\n",
      "\n",
      "                      1\n",
      "\n",
      "   From fairest creatures we desire increase,\n",
      "\n",
      "   That thereby beauty's rose might never die,\n",
      "\n",
      "   But as the riper should by time decease,\n",
      "\n",
      "   His tender heir might bear his memory:\n",
      "\n",
      "   But thou contracted to thine own bright eyes,\n",
      "\n",
      "   Feed'st thy light's flame with self-substantial fuel,\n",
      "\n",
      "114638\n"
     ]
    }
   ],
   "source": [
    "stop_character = charmap['\\n']\n",
    "space_character = charmap[\" \"]\n",
    "lines = np.split(shakespeare_array, np.where(shakespeare_array == stop_character)[0]+1) # split the data in lines\n",
    "shakespeare_lines = []\n",
    "for s in lines:\n",
    "    s_trimmed = np.trim_zeros(s-space_character)+space_character # remove space-only lines\n",
    "    if len(s_trimmed)>1:\n",
    "        shakespeare_lines.append(s)\n",
    "for i in range(10):\n",
    "    print(sh.to_text(shakespeare_lines[i],chars))\n",
    "print(len(shakespeare_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinesDataset(Dataset):\n",
    "    def __init__(self,lines):\n",
    "        self.lines=[torch.tensor(l) for l in lines]\n",
    "    def __getitem__(self,i):\n",
    "        line = self.lines[i]\n",
    "        return line[:-1].to(DEVICE),line[1:].to(DEVICE)\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "# collate fn lets you control the return value of each batch\n",
    "# for packed_seqs, you want to return your data sorted by length\n",
    "def collate_lines(seq_list):\n",
    "    inputs,targets = zip(*seq_list)\n",
    "    lens = [len(seq) for seq in inputs]\n",
    "    seq_order = sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    inputs = [inputs[i] for i in seq_order]\n",
    "    targets = [targets[i] for i in seq_order]\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model that takes packed sequences in training\n",
    "class PackedLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embed_size,hidden_size, nlayers, stop):\n",
    "        super(PackedLanguageModel,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.nlayers=nlayers\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers) # 1 layer, batch_size = False\n",
    "        self.scoring = nn.Linear(hidden_size,vocab_size)\n",
    "        self.stop = stop # stop line character (\\n)\n",
    "    \n",
    "    def forward(self,seq_list): # list\n",
    "        batch_size = len(seq_list)\n",
    "        lens = [len(s) for s in seq_list] # lens of all lines (already sorted)\n",
    "        bounds = [0]\n",
    "        for l in lens:\n",
    "            bounds.append(bounds[-1]+l) # bounds of all lines in the concatenated sequence\n",
    "        seq_concat = torch.cat(seq_list) # concatenated sequence\n",
    "        embed_concat = self.embedding(seq_concat) # concatenated embeddings\n",
    "        embed_list = [embed_concat[bounds[i]:bounds[i+1]] for i in range(batch_size)] # embeddings per line\n",
    "        packed_input = rnn.pack_sequence(embed_list) # packed version\n",
    "        hidden = None\n",
    "        output_packed,hidden = self.rnn(packed_input,hidden)\n",
    "        output_padded, _ = rnn.pad_packed_sequence(output_packed) # unpacked output (padded)\n",
    "        output_flatten = torch.cat([output_padded[:lens[i],i] for i in range(batch_size)]) # concatenated output\n",
    "        scores_flatten = self.scoring(output_flatten) # concatenated logits\n",
    "        return scores_flatten # return concatenated logits\n",
    "    \n",
    "    def generate(self,seq, n_words): # L x V\n",
    "        generated_words = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        hidden = None\n",
    "        output_lstm, hidden = self.rnn(embed,hidden) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        scores = self.scoring(output) # 1 x V\n",
    "        _,current_word = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_words.append(current_word)\n",
    "        if n_words > 1:\n",
    "            for i in range(n_words-1):\n",
    "                embed = self.embedding(current_word).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed,hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                scores = self.scoring(output) # V\n",
    "                _,current_word = torch.max(scores,dim=1) # 1\n",
    "                generated_words.append(current_word)\n",
    "                if current_word[0].item()==self.stop: # If end of line\n",
    "                    break\n",
    "        return torch.cat(generated_words,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_packed(model, optimizer, train_loader, val_loader):\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\") # sum instead of averaging, to take into account the different lengths\n",
    "    criterion = criterion.to(DEVICE)\n",
    "    batch_id=0\n",
    "    before = time.time()\n",
    "    print(\"Training\", len(train_loader), \"number of batches\")\n",
    "    for inputs,targets in train_loader: # lists, presorted, preloaded on GPU\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets)) # criterion of the concatenated output\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % 100 == 0:\n",
    "            after = time.time()\n",
    "            nwords = np.sum(np.array([len(l) for l in inputs]))\n",
    "            lpw = loss.item() / nwords\n",
    "            print(\"Time elapsed: \", after - before)\n",
    "            print(\"At batch\",batch_id)\n",
    "            print(\"Training loss per word:\",lpw)\n",
    "            print(\"Training perplexity :\",np.exp(lpw))\n",
    "            before = after\n",
    "    \n",
    "    val_loss = 0\n",
    "    batch_id=0\n",
    "    nwords = 0\n",
    "    for inputs,targets in val_loader:\n",
    "        nwords += np.sum(np.array([len(l) for l in inputs]))\n",
    "        batch_id+=1\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,torch.cat(targets))\n",
    "        val_loss+=loss.item()\n",
    "    val_lpw = val_loss / nwords\n",
    "    print(\"\\nValidation loss per word:\",val_lpw)\n",
    "    print(\"Validation perplexity :\",np.exp(val_lpw),\"\\n\")\n",
    "    return val_lpw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PackedLanguageModel(charcount,256,256,3, stop=stop_character)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001, weight_decay=1e-6)\n",
    "split = 100000\n",
    "train_dataset = LinesDataset(shakespeare_lines[:split])\n",
    "val_dataset = LinesDataset(shakespeare_lines[split:])\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn = collate_lines)\n",
    "val_loader = DataLoader(val_dataset, shuffle=False, batch_size=64, collate_fn = collate_lines, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1563 number of batches\n",
      "Time elapsed:  8.641294002532959\n",
      "At batch 100\n",
      "Training loss per word: 2.742883137914904\n",
      "Training perplexity : 15.531700639823379\n",
      "Time elapsed:  8.498454809188843\n",
      "At batch 200\n",
      "Training loss per word: 2.3086599491908766\n",
      "Training perplexity : 10.060933455568435\n",
      "Time elapsed:  8.486337661743164\n",
      "At batch 300\n",
      "Training loss per word: 2.0011692090510236\n",
      "Training perplexity : 7.39770050277287\n",
      "Time elapsed:  8.501046419143677\n",
      "At batch 400\n",
      "Training loss per word: 1.8189667917986516\n",
      "Training perplexity : 6.1654849282699615\n",
      "Time elapsed:  8.46365237236023\n",
      "At batch 500\n",
      "Training loss per word: 1.8059468757545878\n",
      "Training perplexity : 6.085731152505224\n",
      "Time elapsed:  8.46724796295166\n",
      "At batch 600\n",
      "Training loss per word: 1.769131321431426\n",
      "Training perplexity : 5.86575569132725\n",
      "Time elapsed:  8.495564222335815\n",
      "At batch 700\n",
      "Training loss per word: 1.7194178916612746\n",
      "Training perplexity : 5.581278609664651\n",
      "Time elapsed:  8.486192464828491\n",
      "At batch 800\n",
      "Training loss per word: 1.6848521969789627\n",
      "Training perplexity : 5.391653970709031\n",
      "Time elapsed:  8.488229751586914\n",
      "At batch 900\n",
      "Training loss per word: 1.6586333367843187\n",
      "Training perplexity : 5.2521280471263845\n",
      "Time elapsed:  8.47211480140686\n",
      "At batch 1000\n",
      "Training loss per word: 1.613853053042763\n",
      "Training perplexity : 5.022124508511031\n",
      "Time elapsed:  8.463880777359009\n",
      "At batch 1100\n",
      "Training loss per word: 1.5037967494708881\n",
      "Training perplexity : 4.498737264325734\n",
      "Time elapsed:  8.485522985458374\n",
      "At batch 1200\n",
      "Training loss per word: 1.5319530345775463\n",
      "Training perplexity : 4.62720509471331\n",
      "Time elapsed:  8.487447261810303\n",
      "At batch 1300\n",
      "Training loss per word: 1.5337706185429474\n",
      "Training perplexity : 4.635623076374849\n",
      "Time elapsed:  8.49773645401001\n",
      "At batch 1400\n",
      "Training loss per word: 1.4681788490536605\n",
      "Training perplexity : 4.341321735310056\n",
      "Time elapsed:  8.445085048675537\n",
      "At batch 1500\n",
      "Training loss per word: 1.4433617978312108\n",
      "Training perplexity : 4.234908820276228\n",
      "\n",
      "Validation loss per word: 1.5670021777555476\n",
      "Validation perplexity : 4.7922602938570975 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.461884498596191\n",
      "At batch 100\n",
      "Training loss per word: 1.4275304737327963\n",
      "Training perplexity : 4.16839251604744\n",
      "Time elapsed:  8.52742624282837\n",
      "At batch 200\n",
      "Training loss per word: 1.3963806072767275\n",
      "Training perplexity : 4.040549135143912\n",
      "Time elapsed:  8.492345333099365\n",
      "At batch 300\n",
      "Training loss per word: 1.3756228943546476\n",
      "Training perplexity : 3.9575410853225157\n",
      "Time elapsed:  8.70175576210022\n",
      "At batch 400\n",
      "Training loss per word: 1.475706498860935\n",
      "Training perplexity : 4.374124995862184\n",
      "Time elapsed:  8.603700160980225\n",
      "At batch 500\n",
      "Training loss per word: 1.3418287228567969\n",
      "Training perplexity : 3.8260338672848966\n",
      "Time elapsed:  8.59308671951294\n",
      "At batch 600\n",
      "Training loss per word: 1.3823432074652777\n",
      "Training perplexity : 3.984226567529948\n",
      "Time elapsed:  8.637450933456421\n",
      "At batch 700\n",
      "Training loss per word: 1.3421172087857083\n",
      "Training perplexity : 3.82713778344363\n",
      "Time elapsed:  8.665077924728394\n",
      "At batch 800\n",
      "Training loss per word: 1.4178690943897954\n",
      "Training perplexity : 4.128314013259831\n",
      "Time elapsed:  8.669797420501709\n",
      "At batch 900\n",
      "Training loss per word: 1.4226571142146018\n",
      "Training perplexity : 4.148127859289235\n",
      "Time elapsed:  8.701568603515625\n",
      "At batch 1000\n",
      "Training loss per word: 1.4568366570035165\n",
      "Training perplexity : 4.292359823035071\n",
      "Time elapsed:  8.64249849319458\n",
      "At batch 1100\n",
      "Training loss per word: 1.3288477701717434\n",
      "Training perplexity : 3.7766892655520463\n",
      "Time elapsed:  8.776566743850708\n",
      "At batch 1200\n",
      "Training loss per word: 1.3541900088922765\n",
      "Training perplexity : 3.8736220861317476\n",
      "Time elapsed:  8.711711645126343\n",
      "At batch 1300\n",
      "Training loss per word: 1.2590422669288572\n",
      "Training perplexity : 3.5220466909522434\n",
      "Time elapsed:  8.609764575958252\n",
      "At batch 1400\n",
      "Training loss per word: 1.34627396466197\n",
      "Training perplexity : 3.8430793706032285\n",
      "Time elapsed:  8.638000011444092\n",
      "At batch 1500\n",
      "Training loss per word: 1.3738777406754032\n",
      "Training perplexity : 3.9506405908996434\n",
      "\n",
      "Validation loss per word: 1.477268740569067\n",
      "Validation perplexity : 4.380963776890887 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.709285259246826\n",
      "At batch 100\n",
      "Training loss per word: 1.2722737889656395\n",
      "Training perplexity : 3.5689584016116584\n",
      "Time elapsed:  8.663956642150879\n",
      "At batch 200\n",
      "Training loss per word: 1.3342647315060945\n",
      "Training perplexity : 3.7972029560476397\n",
      "Time elapsed:  8.721901416778564\n",
      "At batch 300\n",
      "Training loss per word: 1.2764232525021295\n",
      "Training perplexity : 3.5837984321521765\n",
      "Time elapsed:  8.679188013076782\n",
      "At batch 400\n",
      "Training loss per word: 1.3315890071571581\n",
      "Training perplexity : 3.787056268563439\n",
      "Time elapsed:  8.645251512527466\n",
      "At batch 500\n",
      "Training loss per word: 1.330889002743198\n",
      "Training perplexity : 3.7844062400835567\n",
      "Time elapsed:  8.620578527450562\n",
      "At batch 600\n",
      "Training loss per word: 1.2573767913631135\n",
      "Training perplexity : 3.516185690278535\n",
      "Time elapsed:  8.526325941085815\n",
      "At batch 700\n",
      "Training loss per word: 1.2803949381510418\n",
      "Training perplexity : 3.5980604563443874\n",
      "Time elapsed:  8.781673431396484\n",
      "At batch 800\n",
      "Training loss per word: 1.3322228405185879\n",
      "Training perplexity : 3.7894573920437624\n",
      "Time elapsed:  8.527132511138916\n",
      "At batch 900\n",
      "Training loss per word: 1.3215304431373143\n",
      "Training perplexity : 3.7491548570810824\n",
      "Time elapsed:  8.728645324707031\n",
      "At batch 1000\n",
      "Training loss per word: 1.3085250265052906\n",
      "Training perplexity : 3.700711233713722\n",
      "Time elapsed:  8.65996265411377\n",
      "At batch 1100\n",
      "Training loss per word: 1.3180718385350063\n",
      "Training perplexity : 3.7362104106020944\n",
      "Time elapsed:  8.521567583084106\n",
      "At batch 1200\n",
      "Training loss per word: 1.3222853993633854\n",
      "Training perplexity : 3.75198637358396\n",
      "Time elapsed:  8.639692783355713\n",
      "At batch 1300\n",
      "Training loss per word: 1.3140358912350312\n",
      "Training perplexity : 3.7211616507114798\n",
      "Time elapsed:  8.585460662841797\n",
      "At batch 1400\n",
      "Training loss per word: 1.2852012582376702\n",
      "Training perplexity : 3.6153955121310344\n",
      "Time elapsed:  8.573115825653076\n",
      "At batch 1500\n",
      "Training loss per word: 1.248920725983596\n",
      "Training perplexity : 3.48657795310067\n",
      "\n",
      "Validation loss per word: 1.4359684615724608\n",
      "Validation perplexity : 4.203714173693788 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.675545692443848\n",
      "At batch 100\n",
      "Training loss per word: 1.2251082577954155\n",
      "Training perplexity : 3.4045346302548785\n",
      "Time elapsed:  8.570785522460938\n",
      "At batch 200\n",
      "Training loss per word: 1.2268084671134662\n",
      "Training perplexity : 3.4103279753109135\n",
      "Time elapsed:  8.63902735710144\n",
      "At batch 300\n",
      "Training loss per word: 1.308864809528023\n",
      "Training perplexity : 3.7019688862153575\n",
      "Time elapsed:  8.53904914855957\n",
      "At batch 400\n",
      "Training loss per word: 1.2682907869983278\n",
      "Training perplexity : 3.5547715052502764\n",
      "Time elapsed:  8.65868330001831\n",
      "At batch 500\n",
      "Training loss per word: 1.3010250707551998\n",
      "Training perplexity : 3.673059884777777\n",
      "Time elapsed:  8.625437498092651\n",
      "At batch 600\n",
      "Training loss per word: 1.2515248307463673\n",
      "Training perplexity : 3.495669199495546\n",
      "Time elapsed:  8.560718297958374\n",
      "At batch 700\n",
      "Training loss per word: 1.2440925583272417\n",
      "Training perplexity : 3.4697847430354742\n",
      "Time elapsed:  8.70840334892273\n",
      "At batch 800\n",
      "Training loss per word: 1.2542105013041815\n",
      "Training perplexity : 3.505070033465447\n",
      "Time elapsed:  8.521148681640625\n",
      "At batch 900\n",
      "Training loss per word: 1.2803800330045447\n",
      "Training perplexity : 3.598006827125857\n",
      "Time elapsed:  8.628667831420898\n",
      "At batch 1000\n",
      "Training loss per word: 1.232775822800394\n",
      "Training perplexity : 3.4307394560616418\n",
      "Time elapsed:  8.609501361846924\n",
      "At batch 1100\n",
      "Training loss per word: 1.1823026118793554\n",
      "Training perplexity : 3.2618763975957052\n",
      "Time elapsed:  8.526943445205688\n",
      "At batch 1200\n",
      "Training loss per word: 1.2522832724439288\n",
      "Training perplexity : 3.498321466445109\n",
      "Time elapsed:  8.60655951499939\n",
      "At batch 1300\n",
      "Training loss per word: 1.273157854044555\n",
      "Training perplexity : 3.5721149882110246\n",
      "Time elapsed:  8.662357568740845\n",
      "At batch 1400\n",
      "Training loss per word: 1.2741097117901006\n",
      "Training perplexity : 3.575516752271425\n",
      "Time elapsed:  8.6436288356781\n",
      "At batch 1500\n",
      "Training loss per word: 1.2638165971335036\n",
      "Training perplexity : 3.5389023099789925\n",
      "\n",
      "Validation loss per word: 1.4051771200739842\n",
      "Validation perplexity : 4.076248662796974 \n",
      "\n",
      "Training 1563 number of batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  8.70168399810791\n",
      "At batch 100\n",
      "Training loss per word: 1.2418934268228337\n",
      "Training perplexity : 3.4621626141972115\n",
      "Time elapsed:  8.626674890518188\n",
      "At batch 200\n",
      "Training loss per word: 1.1778679438481208\n",
      "Training perplexity : 3.2474430857409047\n",
      "Time elapsed:  8.595263719558716\n",
      "At batch 300\n",
      "Training loss per word: 1.2199829023362427\n",
      "Training perplexity : 3.3871298211194683\n",
      "Time elapsed:  8.714223146438599\n",
      "At batch 400\n",
      "Training loss per word: 1.1951881521072014\n",
      "Training perplexity : 3.3041794003361007\n",
      "Time elapsed:  8.538036823272705\n",
      "At batch 500\n",
      "Training loss per word: 1.2012900288384414\n",
      "Training perplexity : 3.324402733129015\n",
      "Time elapsed:  8.50441312789917\n",
      "At batch 600\n",
      "Training loss per word: 1.229068253391473\n",
      "Training perplexity : 3.418043301884305\n",
      "Time elapsed:  8.59624719619751\n",
      "At batch 700\n",
      "Training loss per word: 1.2383571552498367\n",
      "Training perplexity : 3.449941089015638\n",
      "Time elapsed:  8.592359781265259\n",
      "At batch 800\n",
      "Training loss per word: 1.226766673653238\n",
      "Training perplexity : 3.4101854488826695\n",
      "Time elapsed:  8.599670886993408\n",
      "At batch 900\n",
      "Training loss per word: 1.2468773468791723\n",
      "Training perplexity : 3.479460826535616\n",
      "Time elapsed:  8.661682605743408\n",
      "At batch 1000\n",
      "Training loss per word: 1.149494376024017\n",
      "Training perplexity : 3.1565964552706762\n",
      "Time elapsed:  8.654935598373413\n",
      "At batch 1100\n",
      "Training loss per word: 1.24389865599876\n",
      "Training perplexity : 3.4691120089189327\n",
      "Time elapsed:  8.576208591461182\n",
      "At batch 1200\n",
      "Training loss per word: 1.1862686894442995\n",
      "Training perplexity : 3.2748389406299045\n",
      "Time elapsed:  8.494652032852173\n",
      "At batch 1300\n",
      "Training loss per word: 1.2427397248067338\n",
      "Training perplexity : 3.465093875622906\n",
      "Time elapsed:  8.48995041847229\n",
      "At batch 1400\n",
      "Training loss per word: 1.2515025053922153\n",
      "Training perplexity : 3.495591158313821\n",
      "Time elapsed:  8.491538763046265\n",
      "At batch 1500\n",
      "Training loss per word: 1.2223317185057745\n",
      "Training perplexity : 3.3950949170227305\n",
      "\n",
      "Validation loss per word: 1.3904597628622668\n",
      "Validation perplexity : 4.016696356344472 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.459374189376831\n",
      "At batch 100\n",
      "Training loss per word: 1.216573491916694\n",
      "Training perplexity : 3.3756013691943374\n",
      "Time elapsed:  8.456801891326904\n",
      "At batch 200\n",
      "Training loss per word: 1.23364844003488\n",
      "Training perplexity : 3.433734485004919\n",
      "Time elapsed:  8.43958568572998\n",
      "At batch 300\n",
      "Training loss per word: 1.2324671011826\n",
      "Training perplexity : 3.4296804760999513\n",
      "Time elapsed:  8.478583097457886\n",
      "At batch 400\n",
      "Training loss per word: 1.204116657896157\n",
      "Training perplexity : 3.333812879725474\n",
      "Time elapsed:  8.49569320678711\n",
      "At batch 500\n",
      "Training loss per word: 1.1646884236420818\n",
      "Training perplexity : 3.204924149026158\n",
      "Time elapsed:  8.48216199874878\n",
      "At batch 600\n",
      "Training loss per word: 1.1666229147328795\n",
      "Training perplexity : 3.211130046930897\n",
      "Time elapsed:  8.471184492111206\n",
      "At batch 700\n",
      "Training loss per word: 1.1737848417637713\n",
      "Training perplexity : 3.2342104775379696\n",
      "Time elapsed:  8.451525449752808\n",
      "At batch 800\n",
      "Training loss per word: 1.1957450685714583\n",
      "Training perplexity : 3.3060200647455895\n",
      "Time elapsed:  8.473682165145874\n",
      "At batch 900\n",
      "Training loss per word: 1.2207735944872682\n",
      "Training perplexity : 3.389809057169402\n",
      "Time elapsed:  8.453316688537598\n",
      "At batch 1000\n",
      "Training loss per word: 1.2488520566336654\n",
      "Training perplexity : 3.486338540279404\n",
      "Time elapsed:  8.45765733718872\n",
      "At batch 1100\n",
      "Training loss per word: 1.1794535222036862\n",
      "Training perplexity : 3.252596245498261\n",
      "Time elapsed:  8.455255270004272\n",
      "At batch 1200\n",
      "Training loss per word: 1.2455993814671293\n",
      "Training perplexity : 3.4750170360568085\n",
      "Time elapsed:  8.4639573097229\n",
      "At batch 1300\n",
      "Training loss per word: 1.2235278239738037\n",
      "Training perplexity : 3.399158238213368\n",
      "Time elapsed:  8.447855710983276\n",
      "At batch 1400\n",
      "Training loss per word: 1.1968963473694942\n",
      "Training perplexity : 3.309828407363423\n",
      "Time elapsed:  8.459014654159546\n",
      "At batch 1500\n",
      "Training loss per word: 1.2274084314123377\n",
      "Training perplexity : 3.412374664252297\n",
      "\n",
      "Validation loss per word: 1.3814297296537972\n",
      "Validation perplexity : 3.9805887267609736 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.463352680206299\n",
      "At batch 100\n",
      "Training loss per word: 1.2041307148733225\n",
      "Training perplexity : 3.3338597433863777\n",
      "Time elapsed:  8.436999320983887\n",
      "At batch 200\n",
      "Training loss per word: 1.2663991893139002\n",
      "Training perplexity : 3.5480536634324054\n",
      "Time elapsed:  8.452497482299805\n",
      "At batch 300\n",
      "Training loss per word: 1.189139742319746\n",
      "Training perplexity : 3.2842546864713222\n",
      "Time elapsed:  8.439671993255615\n",
      "At batch 400\n",
      "Training loss per word: 1.2107958061868165\n",
      "Training perplexity : 3.3561544385581286\n",
      "Time elapsed:  8.434110164642334\n",
      "At batch 500\n",
      "Training loss per word: 1.1928454509710065\n",
      "Training perplexity : 3.296447755503407\n",
      "Time elapsed:  8.433535099029541\n",
      "At batch 600\n",
      "Training loss per word: 1.2354188881876742\n",
      "Training perplexity : 3.4398191185558473\n",
      "Time elapsed:  8.472768068313599\n",
      "At batch 700\n",
      "Training loss per word: 1.174934760340791\n",
      "Training perplexity : 3.237931695386798\n",
      "Time elapsed:  8.43019437789917\n",
      "At batch 800\n",
      "Training loss per word: 1.151162085245323\n",
      "Training perplexity : 3.1618651323766427\n",
      "Time elapsed:  8.435695171356201\n",
      "At batch 900\n",
      "Training loss per word: 1.248876222212618\n",
      "Training perplexity : 3.48642279068663\n",
      "Time elapsed:  8.472634077072144\n",
      "At batch 1000\n",
      "Training loss per word: 1.2130786984465842\n",
      "Training perplexity : 3.3638249296695992\n",
      "Time elapsed:  8.44743275642395\n",
      "At batch 1100\n",
      "Training loss per word: 1.1568667207760883\n",
      "Training perplexity : 3.1799539666023198\n",
      "Time elapsed:  8.46513319015503\n",
      "At batch 1200\n",
      "Training loss per word: 1.169309911948226\n",
      "Training perplexity : 3.219769946920416\n",
      "Time elapsed:  8.452090978622437\n",
      "At batch 1300\n",
      "Training loss per word: 1.1797493644382642\n",
      "Training perplexity : 3.253558643191632\n",
      "Time elapsed:  8.418010711669922\n",
      "At batch 1400\n",
      "Training loss per word: 1.1725885671300134\n",
      "Training perplexity : 3.2303437868565728\n",
      "Time elapsed:  8.457894802093506\n",
      "At batch 1500\n",
      "Training loss per word: 1.2237137083023313\n",
      "Training perplexity : 3.3997901471892056\n",
      "\n",
      "Validation loss per word: 1.362282631558164\n",
      "Validation perplexity : 3.9050970336862942 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.617129564285278\n",
      "At batch 100\n",
      "Training loss per word: 1.202888845656485\n",
      "Training perplexity : 3.329722095338453\n",
      "Time elapsed:  8.49680495262146\n",
      "At batch 200\n",
      "Training loss per word: 1.1446720296738557\n",
      "Training perplexity : 3.1414108983116216\n",
      "Time elapsed:  8.489643335342407\n",
      "At batch 300\n",
      "Training loss per word: 1.1983554610377591\n",
      "Training perplexity : 3.3146613482798437\n",
      "Time elapsed:  8.483105182647705\n",
      "At batch 400\n",
      "Training loss per word: 1.1026628039679915\n",
      "Training perplexity : 3.0121761891723544\n",
      "Time elapsed:  8.462969303131104\n",
      "At batch 500\n",
      "Training loss per word: 1.1977096681270258\n",
      "Training perplexity : 3.3125214545181234\n",
      "Time elapsed:  8.487247228622437\n",
      "At batch 600\n",
      "Training loss per word: 1.1669476039217077\n",
      "Training perplexity : 3.2121728354229706\n",
      "Time elapsed:  8.499165058135986\n",
      "At batch 700\n",
      "Training loss per word: 1.1691870762343533\n",
      "Training perplexity : 3.2193744684704164\n",
      "Time elapsed:  8.500579357147217\n",
      "At batch 800\n",
      "Training loss per word: 1.1899058006957308\n",
      "Training perplexity : 3.286771581203821\n",
      "Time elapsed:  8.498432874679565\n",
      "At batch 900\n",
      "Training loss per word: 1.1527254255871586\n",
      "Training perplexity : 3.1668120695592235\n",
      "Time elapsed:  8.47116732597351\n",
      "At batch 1000\n",
      "Training loss per word: 1.1906302902611878\n",
      "Training perplexity : 3.2891536757155584\n",
      "Time elapsed:  8.498794078826904\n",
      "At batch 1100\n",
      "Training loss per word: 1.131616488720765\n",
      "Training perplexity : 3.100664640651325\n",
      "Time elapsed:  8.476269721984863\n",
      "At batch 1200\n",
      "Training loss per word: 1.171387978505547\n",
      "Training perplexity : 3.2264678000513656\n",
      "Time elapsed:  8.496475219726562\n",
      "At batch 1300\n",
      "Training loss per word: 1.1782232453518184\n",
      "Training perplexity : 3.2485971121539605\n",
      "Time elapsed:  8.463498830795288\n",
      "At batch 1400\n",
      "Training loss per word: 1.1780820332202\n",
      "Training perplexity : 3.248138403219379\n",
      "Time elapsed:  8.501079559326172\n",
      "At batch 1500\n",
      "Training loss per word: 1.1704956748964954\n",
      "Training perplexity : 3.223590095273008\n",
      "\n",
      "Validation loss per word: 1.355738828930999\n",
      "Validation perplexity : 3.8796262781473825 \n",
      "\n",
      "Training 1563 number of batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  8.480568885803223\n",
      "At batch 100\n",
      "Training loss per word: 1.192062441325104\n",
      "Training perplexity : 3.29386761538276\n",
      "Time elapsed:  8.735455751419067\n",
      "At batch 200\n",
      "Training loss per word: 1.2190057841399802\n",
      "Training perplexity : 3.38382181135961\n",
      "Time elapsed:  8.494828462600708\n",
      "At batch 300\n",
      "Training loss per word: 1.1886550210449385\n",
      "Training perplexity : 3.282663124116132\n",
      "Time elapsed:  8.451841354370117\n",
      "At batch 400\n",
      "Training loss per word: 1.129733033466058\n",
      "Training perplexity : 3.094830173744251\n",
      "Time elapsed:  8.485805988311768\n",
      "At batch 500\n",
      "Training loss per word: 1.1904845342010437\n",
      "Training perplexity : 3.2886742965716342\n",
      "Time elapsed:  8.504512786865234\n",
      "At batch 600\n",
      "Training loss per word: 1.1841058020113673\n",
      "Training perplexity : 3.2677634871033923\n",
      "Time elapsed:  8.495721340179443\n",
      "At batch 700\n",
      "Training loss per word: 1.166028470087904\n",
      "Training perplexity : 3.2092217751073555\n",
      "Time elapsed:  8.496970415115356\n",
      "At batch 800\n",
      "Training loss per word: 1.1948274427925882\n",
      "Training perplexity : 3.302987766978793\n",
      "Time elapsed:  8.466972827911377\n",
      "At batch 900\n",
      "Training loss per word: 1.2014057084908634\n",
      "Training perplexity : 3.324787321125768\n",
      "Time elapsed:  8.458056688308716\n",
      "At batch 1000\n",
      "Training loss per word: 1.1630822084193813\n",
      "Training perplexity : 3.1997804830935093\n",
      "Time elapsed:  8.488721370697021\n",
      "At batch 1100\n",
      "Training loss per word: 1.1925782561702485\n",
      "Training perplexity : 3.2955670794634355\n",
      "Time elapsed:  8.4820876121521\n",
      "At batch 1200\n",
      "Training loss per word: 1.1679885727144175\n",
      "Training perplexity : 3.215518348086471\n",
      "Time elapsed:  8.44859266281128\n",
      "At batch 1300\n",
      "Training loss per word: 1.163525012749201\n",
      "Training perplexity : 3.2011976734917615\n",
      "Time elapsed:  8.577474117279053\n",
      "At batch 1400\n",
      "Training loss per word: 1.1958918580396132\n",
      "Training perplexity : 3.3065053892919973\n",
      "Time elapsed:  8.474345684051514\n",
      "At batch 1500\n",
      "Training loss per word: 1.1635542857496044\n",
      "Training perplexity : 3.2012913835241297\n",
      "\n",
      "Validation loss per word: 1.3533579173384829\n",
      "Validation perplexity : 3.8704002185415893 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.475265741348267\n",
      "At batch 100\n",
      "Training loss per word: 1.1586507848095258\n",
      "Training perplexity : 3.1856322718260666\n",
      "Time elapsed:  8.470481634140015\n",
      "At batch 200\n",
      "Training loss per word: 1.1510734046818814\n",
      "Training perplexity : 3.161584748827646\n",
      "Time elapsed:  8.480122566223145\n",
      "At batch 300\n",
      "Training loss per word: 1.0768561123109581\n",
      "Training perplexity : 2.935436346688321\n",
      "Time elapsed:  8.461222887039185\n",
      "At batch 400\n",
      "Training loss per word: 1.1374553236742713\n",
      "Training perplexity : 3.118821866672597\n",
      "Time elapsed:  8.466567993164062\n",
      "At batch 500\n",
      "Training loss per word: 1.1241843028708183\n",
      "Training perplexity : 3.077705349327869\n",
      "Time elapsed:  8.44620966911316\n",
      "At batch 600\n",
      "Training loss per word: 1.1420158592975462\n",
      "Training perplexity : 3.13307784764418\n",
      "Time elapsed:  8.456995010375977\n",
      "At batch 700\n",
      "Training loss per word: 1.0990278132329747\n",
      "Training perplexity : 3.001246832721467\n",
      "Time elapsed:  8.436715364456177\n",
      "At batch 800\n",
      "Training loss per word: 1.240357116489476\n",
      "Training perplexity : 3.4568477416894017\n",
      "Time elapsed:  8.434327363967896\n",
      "At batch 900\n",
      "Training loss per word: 1.1651662457051701\n",
      "Training perplexity : 3.2064558984177713\n",
      "Time elapsed:  8.414306879043579\n",
      "At batch 1000\n",
      "Training loss per word: 1.1327477608632917\n",
      "Training perplexity : 3.1041743210101203\n",
      "Time elapsed:  8.458057403564453\n",
      "At batch 1100\n",
      "Training loss per word: 1.1685057368452008\n",
      "Training perplexity : 3.217181728921355\n",
      "Time elapsed:  8.501013278961182\n",
      "At batch 1200\n",
      "Training loss per word: 1.064974728370703\n",
      "Training perplexity : 2.9007656760584886\n",
      "Time elapsed:  8.500956058502197\n",
      "At batch 1300\n",
      "Training loss per word: 1.1952902231954967\n",
      "Training perplexity : 3.30451667873631\n",
      "Time elapsed:  8.496525764465332\n",
      "At batch 1400\n",
      "Training loss per word: 1.1597220924292264\n",
      "Training perplexity : 3.189046892680528\n",
      "Time elapsed:  8.508332014083862\n",
      "At batch 1500\n",
      "Training loss per word: 1.1104116491238503\n",
      "Training perplexity : 3.0356077425390766\n",
      "\n",
      "Validation loss per word: 1.3473911162937449\n",
      "Validation perplexity : 3.8473750720213 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.652124643325806\n",
      "At batch 100\n",
      "Training loss per word: 1.1845835357635406\n",
      "Training perplexity : 3.269324980974766\n",
      "Time elapsed:  8.590143918991089\n",
      "At batch 200\n",
      "Training loss per word: 1.1592656020894427\n",
      "Training perplexity : 3.1875914558026137\n",
      "Time elapsed:  8.647367000579834\n",
      "At batch 300\n",
      "Training loss per word: 1.1178202127909158\n",
      "Training perplexity : 3.058180749300177\n",
      "Time elapsed:  8.579856395721436\n",
      "At batch 400\n",
      "Training loss per word: 1.195169078016115\n",
      "Training perplexity : 3.304116376718314\n",
      "Time elapsed:  8.5316641330719\n",
      "At batch 500\n",
      "Training loss per word: 1.1232531012997091\n",
      "Training perplexity : 3.0748407192521796\n",
      "Time elapsed:  8.64833116531372\n",
      "At batch 600\n",
      "Training loss per word: 1.1679908948066906\n",
      "Training perplexity : 3.21552581482545\n",
      "Time elapsed:  8.553852319717407\n",
      "At batch 700\n",
      "Training loss per word: 1.1016589641724222\n",
      "Training perplexity : 3.0091539640111304\n",
      "Time elapsed:  8.697795152664185\n",
      "At batch 800\n",
      "Training loss per word: 1.1246018945970118\n",
      "Training perplexity : 3.078990842004282\n",
      "Time elapsed:  8.575396537780762\n",
      "At batch 900\n",
      "Training loss per word: 1.1137363267867848\n",
      "Training perplexity : 3.0457169554194223\n",
      "Time elapsed:  8.623439311981201\n",
      "At batch 1000\n",
      "Training loss per word: 1.2195494768278494\n",
      "Training perplexity : 3.385662070757956\n",
      "Time elapsed:  8.58987021446228\n",
      "At batch 1100\n",
      "Training loss per word: 1.1298067648979107\n",
      "Training perplexity : 3.0950583684167583\n",
      "Time elapsed:  8.614128112792969\n",
      "At batch 1200\n",
      "Training loss per word: 1.1190051508884804\n",
      "Training perplexity : 3.061806651990125\n",
      "Time elapsed:  8.548489093780518\n",
      "At batch 1300\n",
      "Training loss per word: 1.1623243472884324\n",
      "Training perplexity : 3.1973564125082645\n",
      "Time elapsed:  8.560014724731445\n",
      "At batch 1400\n",
      "Training loss per word: 1.1734078198381415\n",
      "Training perplexity : 3.232991339111236\n",
      "Time elapsed:  8.700252056121826\n",
      "At batch 1500\n",
      "Training loss per word: 1.089664296875\n",
      "Training perplexity : 2.9732757670388175\n",
      "\n",
      "Validation loss per word: 1.3522964946832685\n",
      "Validation perplexity : 3.866294267525084 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.602701663970947\n",
      "At batch 100\n",
      "Training loss per word: 1.0997401165413152\n",
      "Training perplexity : 3.0033853923306824\n",
      "Time elapsed:  8.748917579650879\n",
      "At batch 200\n",
      "Training loss per word: 1.1911726600979107\n",
      "Training perplexity : 3.2909380973220976\n",
      "Time elapsed:  8.660587549209595\n",
      "At batch 300\n",
      "Training loss per word: 1.1595541037842554\n",
      "Training perplexity : 3.188511214009504\n",
      "Time elapsed:  8.652813911437988\n",
      "At batch 400\n",
      "Training loss per word: 1.160019004737938\n",
      "Training perplexity : 3.1899939005382105\n",
      "Time elapsed:  8.625682353973389\n",
      "At batch 500\n",
      "Training loss per word: 1.0888902341291693\n",
      "Training perplexity : 2.970975155558163\n",
      "Time elapsed:  8.675954341888428\n",
      "At batch 600\n",
      "Training loss per word: 1.1001884242466518\n",
      "Training perplexity : 3.0047321349991614\n",
      "Time elapsed:  8.532971143722534\n",
      "At batch 700\n",
      "Training loss per word: 1.1159848078993337\n",
      "Training perplexity : 3.0525728973073285\n",
      "Time elapsed:  8.739808320999146\n",
      "At batch 800\n",
      "Training loss per word: 1.1065473751145967\n",
      "Training perplexity : 3.023899958049929\n",
      "Time elapsed:  8.665616512298584\n",
      "At batch 900\n",
      "Training loss per word: 1.107960625\n",
      "Training perplexity : 3.02817650552244\n",
      "Time elapsed:  8.653008460998535\n",
      "At batch 1000\n",
      "Training loss per word: 1.0793123090883847\n",
      "Training perplexity : 2.9426552178381966\n",
      "Time elapsed:  8.557585716247559\n",
      "At batch 1100\n",
      "Training loss per word: 1.1957349976280836\n",
      "Training perplexity : 3.3059867701723755\n",
      "Time elapsed:  8.679403066635132\n",
      "At batch 1200\n",
      "Training loss per word: 1.1534233725252652\n",
      "Training perplexity : 3.1690231078506788\n",
      "Time elapsed:  8.607325315475464\n",
      "At batch 1300\n",
      "Training loss per word: 1.146721161028569\n",
      "Training perplexity : 3.147854661685428\n",
      "Time elapsed:  8.52121090888977\n",
      "At batch 1400\n",
      "Training loss per word: 1.0417493077691253\n",
      "Training perplexity : 2.834170516992458\n",
      "Time elapsed:  8.67890977859497\n",
      "At batch 1500\n",
      "Training loss per word: 1.1968514935234666\n",
      "Training perplexity : 3.3096799521590796\n",
      "\n",
      "Validation loss per word: 1.3472457519723537\n",
      "Validation perplexity : 3.8468158416018787 \n",
      "\n",
      "Training 1563 number of batches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed:  8.602280616760254\n",
      "At batch 100\n",
      "Training loss per word: 1.1490344318115462\n",
      "Training perplexity : 3.1551449308356894\n",
      "Time elapsed:  8.57916784286499\n",
      "At batch 200\n",
      "Training loss per word: 1.1811499477844756\n",
      "Training perplexity : 3.258118715878707\n",
      "Time elapsed:  8.544205904006958\n",
      "At batch 300\n",
      "Training loss per word: 1.108135528908009\n",
      "Training perplexity : 3.0287061917481424\n",
      "Time elapsed:  8.637030601501465\n",
      "At batch 400\n",
      "Training loss per word: 1.1722897607514646\n",
      "Training perplexity : 3.2293786837248213\n",
      "Time elapsed:  8.527188062667847\n",
      "At batch 500\n",
      "Training loss per word: 1.0946651855164904\n",
      "Training perplexity : 2.988182029263203\n",
      "Time elapsed:  8.53776741027832\n",
      "At batch 600\n",
      "Training loss per word: 1.1453837968438745\n",
      "Training perplexity : 3.143647647384427\n",
      "Time elapsed:  8.647263765335083\n",
      "At batch 700\n",
      "Training loss per word: 1.1851636104313987\n",
      "Training perplexity : 3.2712219837256526\n",
      "Time elapsed:  8.71172308921814\n",
      "At batch 800\n",
      "Training loss per word: 1.072200887678157\n",
      "Training perplexity : 2.921802988891551\n",
      "Time elapsed:  8.59160590171814\n",
      "At batch 900\n",
      "Training loss per word: 1.0866779553333594\n",
      "Training perplexity : 2.9644097951001775\n",
      "Time elapsed:  8.550717830657959\n",
      "At batch 1000\n",
      "Training loss per word: 1.1117617548728476\n",
      "Training perplexity : 3.03970890188042\n",
      "Time elapsed:  8.622942447662354\n",
      "At batch 1100\n",
      "Training loss per word: 1.1151592674499327\n",
      "Training perplexity : 3.050053914809888\n",
      "Time elapsed:  8.658621311187744\n",
      "At batch 1200\n",
      "Training loss per word: 1.1896884363854097\n",
      "Training perplexity : 3.2860572320057124\n",
      "Time elapsed:  8.771921634674072\n",
      "At batch 1300\n",
      "Training loss per word: 1.1629021349292117\n",
      "Training perplexity : 3.199204339329808\n",
      "Time elapsed:  8.619447469711304\n",
      "At batch 1400\n",
      "Training loss per word: 1.1672126090172494\n",
      "Training perplexity : 3.213024190393839\n",
      "Time elapsed:  8.596953868865967\n",
      "At batch 1500\n",
      "Training loss per word: 1.1576064446653413\n",
      "Training perplexity : 3.1823071247544883\n",
      "\n",
      "Validation loss per word: 1.346994945049164\n",
      "Validation perplexity : 3.8458511545367235 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.645659446716309\n",
      "At batch 100\n",
      "Training loss per word: 1.1439508058708912\n",
      "Training perplexity : 3.139146054824557\n",
      "Time elapsed:  8.546679258346558\n",
      "At batch 200\n",
      "Training loss per word: 1.0243465628223865\n",
      "Training perplexity : 2.78527486366199\n",
      "Time elapsed:  8.51897931098938\n",
      "At batch 300\n",
      "Training loss per word: 1.1278586387634277\n",
      "Training perplexity : 3.0890346736851533\n",
      "Time elapsed:  8.482156038284302\n",
      "At batch 400\n",
      "Training loss per word: 1.0524466740145229\n",
      "Training perplexity : 2.86465141903143\n",
      "Time elapsed:  8.503598690032959\n",
      "At batch 500\n",
      "Training loss per word: 1.0974335044488224\n",
      "Training perplexity : 2.9964657308217535\n",
      "Time elapsed:  8.516186475753784\n",
      "At batch 600\n",
      "Training loss per word: 1.11737091825568\n",
      "Training perplexity : 3.0568070340262445\n",
      "Time elapsed:  8.487314701080322\n",
      "At batch 700\n",
      "Training loss per word: 1.1382212205926898\n",
      "Training perplexity : 3.121211477710431\n",
      "Time elapsed:  8.496089220046997\n",
      "At batch 800\n",
      "Training loss per word: 1.1896058881670788\n",
      "Training perplexity : 3.285785985031508\n",
      "Time elapsed:  8.494760274887085\n",
      "At batch 900\n",
      "Training loss per word: 1.080539597716072\n",
      "Training perplexity : 2.9462689221977953\n",
      "Time elapsed:  8.506214618682861\n",
      "At batch 1000\n",
      "Training loss per word: 1.1407863573096264\n",
      "Training perplexity : 3.1292280893299096\n",
      "Time elapsed:  8.507078409194946\n",
      "At batch 1100\n",
      "Training loss per word: 1.160974349975586\n",
      "Training perplexity : 3.1930429022119324\n",
      "Time elapsed:  8.482260942459106\n",
      "At batch 1200\n",
      "Training loss per word: 1.1255660187251983\n",
      "Training perplexity : 3.0819608028409182\n",
      "Time elapsed:  8.494476795196533\n",
      "At batch 1300\n",
      "Training loss per word: 1.1453736363754146\n",
      "Training perplexity : 3.1436157066139234\n",
      "Time elapsed:  8.502600193023682\n",
      "At batch 1400\n",
      "Training loss per word: 1.144893079750782\n",
      "Training perplexity : 3.1421053841876003\n",
      "Time elapsed:  8.522473335266113\n",
      "At batch 1500\n",
      "Training loss per word: 1.1001486466250827\n",
      "Training perplexity : 3.00461261627848\n",
      "\n",
      "Validation loss per word: 1.3428427836942103\n",
      "Validation perplexity : 3.829915666249838 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.495575428009033\n",
      "At batch 100\n",
      "Training loss per word: 1.1206689264466982\n",
      "Training perplexity : 3.0669050511815814\n",
      "Time elapsed:  8.511616945266724\n",
      "At batch 200\n",
      "Training loss per word: 1.1394691807320898\n",
      "Training perplexity : 3.1251090567270357\n",
      "Time elapsed:  8.504302978515625\n",
      "At batch 300\n",
      "Training loss per word: 1.1216985824260306\n",
      "Training perplexity : 3.070064534616942\n",
      "Time elapsed:  8.522451400756836\n",
      "At batch 400\n",
      "Training loss per word: 1.1858039298646095\n",
      "Training perplexity : 3.273317281490526\n",
      "Time elapsed:  8.487694025039673\n",
      "At batch 500\n",
      "Training loss per word: 1.1269932895351802\n",
      "Training perplexity : 3.086362736160523\n",
      "Time elapsed:  8.513871669769287\n",
      "At batch 600\n",
      "Training loss per word: 1.1347784917600285\n",
      "Training perplexity : 3.110484468650023\n",
      "Time elapsed:  8.479548692703247\n",
      "At batch 700\n",
      "Training loss per word: 1.0822763043871215\n",
      "Training perplexity : 2.951390172857569\n",
      "Time elapsed:  8.504786729812622\n",
      "At batch 800\n",
      "Training loss per word: 1.182099841889881\n",
      "Training perplexity : 3.26121505400548\n",
      "Time elapsed:  8.524677276611328\n",
      "At batch 900\n",
      "Training loss per word: 1.0801827107600017\n",
      "Training perplexity : 2.9452176248586994\n",
      "Time elapsed:  8.509321212768555\n",
      "At batch 1000\n",
      "Training loss per word: 1.1258139087066001\n",
      "Training perplexity : 3.0827248847472277\n",
      "Time elapsed:  8.500277280807495\n",
      "At batch 1100\n",
      "Training loss per word: 1.130855161230847\n",
      "Training perplexity : 3.0983049177982975\n",
      "Time elapsed:  8.51124882698059\n",
      "At batch 1200\n",
      "Training loss per word: 1.1233390471299185\n",
      "Training perplexity : 3.0751050003473237\n",
      "Time elapsed:  8.503670454025269\n",
      "At batch 1300\n",
      "Training loss per word: 1.0567892323369565\n",
      "Training perplexity : 2.8771183845628423\n",
      "Time elapsed:  8.490436792373657\n",
      "At batch 1400\n",
      "Training loss per word: 1.0824269536344555\n",
      "Training perplexity : 2.95183483105857\n",
      "Time elapsed:  8.513868570327759\n",
      "At batch 1500\n",
      "Training loss per word: 1.1917382245829304\n",
      "Training perplexity : 3.292799861456553\n",
      "\n",
      "Validation loss per word: 1.3417807137435196\n",
      "Validation perplexity : 3.8258501872007535 \n",
      "\n",
      "Training 1563 number of batches\n",
      "Time elapsed:  8.486592769622803\n",
      "At batch 100\n",
      "Training loss per word: 1.0728940970065026\n",
      "Training perplexity : 2.923829112161678\n",
      "Time elapsed:  8.477813243865967\n",
      "At batch 200\n",
      "Training loss per word: 1.102588640414447\n",
      "Training perplexity : 3.0119528037658934\n",
      "Time elapsed:  8.516963481903076\n",
      "At batch 300\n",
      "Training loss per word: 1.1874646968890585\n",
      "Training perplexity : 3.278758015537503\n",
      "Time elapsed:  8.500415563583374\n",
      "At batch 400\n",
      "Training loss per word: 1.1350421623643396\n",
      "Training perplexity : 3.111304720102918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f9019b9d595c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_epoch_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"packed_model_epoch_20.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-607c25ff449c>\u001b[0m in \u001b[0;36mtrain_epoch_packed\u001b[0;34m(model, optimizer, train_loader, val_loader)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# criterion of the concatenated output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    train_epoch_packed(model, optimizer, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type PackedLanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"trained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uarrel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"To be, or not to be, that is the q\",20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    The sea of the sea of the strength of the streets\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Richard \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, \"Hello\", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
