{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a python framework for machine learning\n",
    "\n",
    "- GPU-accelerated computations\n",
    "- automatic differentiation\n",
    "- modules for neural networks\n",
    "\n",
    "This tutorial will teach you the fundamentals of operating on pytorch tensors and networks. You have already seen some things in recitation 0 which we will quickly review, but most of this tutorial is on mostly new or more advanced stuff.\n",
    "\n",
    "For a worked example of how to build and train a pytorch network, see `pytorch-example.ipynb`.\n",
    "\n",
    "For additional tutorials, see http://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are the fundamental object for array data. The most common types you will use are `IntTensor` and `FloatTensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  3.3967e-17],\n",
      "        [-4.6577e-10,  1.1003e+24,  1.2944e-08]])\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create uninitialized tensor\n",
    "x = torch.FloatTensor(2,3)\n",
    "print(x)\n",
    "# Initialize to zeros\n",
    "x.zero_()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6965,  0.2861,  0.2269],\n",
      "        [ 0.5513,  0.7195,  0.4231]])\n",
      "tensor([[ 0.6965,  0.2861,  0.2269],\n",
      "        [ 0.5513,  0.7195,  0.4231]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Create from numpy array (seed for repeatability)\n",
    "np.random.seed(123)\n",
    "np_array = np.random.random((2,3))\n",
    "print(torch.FloatTensor(np_array))\n",
    "print(torch.from_numpy(np_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696],\n",
      "        [-0.2404, -1.1969,  0.2093]])\n",
      "[[-0.11146712  0.12036294 -0.3696345 ]\n",
      " [-0.24041797 -1.1969243   0.20926936]]\n"
     ]
    }
   ],
   "source": [
    "# Create random tensor (seed for repeatability)\n",
    "torch.manual_seed(123)\n",
    "x=torch.randn(2,3)\n",
    "print(x)\n",
    "# export to numpy array\n",
    "x_np = x.numpy()\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  0.,  1.]])\n",
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]])\n",
      "tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.]])\n",
      "tensor([ 0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "# special tensors (see documentation)\n",
    "print(torch.eye(3))\n",
    "print(torch.ones(2,3))\n",
    "print(torch.zeros(2,3))\n",
    "print(torch.arange(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tensors have a `size` and `type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x=torch.FloatTensor(3,4)\n",
    "print(x.size())\n",
    "print(x.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math, Linear Algebra, and Indexing (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch math and linear algebra is similar to numpy. Operators are overridden so you can use standard math operators (`+`,`-`, etc.) and expect a tensor as a result. See pytorch documentation for a complete list of available functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(85.7910)\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,5)\n",
    "print(torch.sum(x))\n",
    "print(torch.sum(torch.exp(x)))\n",
    "print(torch.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch indexing is similar to numpy indexing. See pytorch documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0756,  0.1966],\n",
      "        [ 0.3164,  0.4017],\n",
      "        [ 0.1186,  0.8274]])\n",
      "tensor([ 0.3164,  0.4017])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,2)\n",
    "print(x)\n",
    "print(x[1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be copied between CPU and GPU. It is important that everything involved in a calculation is on the same device. \n",
    "\n",
    "This portion of the tutorial may not work for you if you do not have a GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tensor\n",
    "x = torch.rand(3,2)\n",
    "print(x)\n",
    "# copy to GPU\n",
    "y = x.cuda()\n",
    "print(y)\n",
    "# copy back to CPU\n",
    "z = y.cpu()\n",
    "print(z)\n",
    "# get CPU tensor as numpy array\n",
    "print(z.numpy())\n",
    "# cannot get GPU tensor as numpy array directly\n",
    "try:\n",
    "  y.numpy()\n",
    "except RuntimeError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations between GPU and CPU tensors will fail. Operations require all arguments to be on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.mm received an invalid combination of arguments - got (torch.FloatTensor, torch.cuda.FloatTensor), but expected one of:\n",
      " * (torch.FloatTensor source, torch.FloatTensor mat2)\n",
      "      didn't match because some of the arguments have invalid types: (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m)\n",
      " * (torch.SparseFloatTensor source, torch.FloatTensor mat2)\n",
      "      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.FloatTensor\u001b[0m, \u001b[31;1mtorch.cuda.FloatTensor\u001b[0m)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,5)  # CPU tensor\n",
    "y = torch.rand(5,4).cuda()  # GPU tensor\n",
    "try:\n",
    "  torch.mm(x,y)  # Operation between CPU and GPU fails\n",
    "except TypeError as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical code should include `if` statements or utilize helper functions so it can operate with or without the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put tensor on CUDA if available\n",
    "x = torch.rand(3,2)\n",
    "if torch.cuda.is_available():\n",
    "  x = x.cuda()\n",
    "\n",
    "# Do some calculations\n",
    "y = x ** 2 \n",
    "\n",
    "# Copy to CPU if on GPU\n",
    "if y.is_cuda:\n",
    "  y = y.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient method is `new`, which creates a new tensor on the same device as another tensor. It should be used for creating tensors whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.4406e+06  4.5734e-41\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "\n",
      " 0.1280  0.5219\n",
      "[torch.cuda.FloatTensor of size 1x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.rand(3,2)\n",
    "x2 = x1.new(1,2)  # create cpu tensor\n",
    "print(x2)\n",
    "x1 = torch.rand(3,2).cuda()\n",
    "x2 = x1.new(1,2)  # create cuda tensor\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculations executed on the GPU can be many times faster than numpy. However, numpy is still optimized for the CPU and many times faster than python `for` loops. Numpy calculations may be faster than GPU calculations for small arrays due to the cost of interfacing with the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 185.08557113818824ms\n",
      "GPU: 52.751455921679735ms\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "# Create random data\n",
    "x = torch.rand(1000,64)\n",
    "y = torch.rand(64,32)\n",
    "number = 10000  # number of iterations\n",
    "\n",
    "def square():\n",
    "  z=torch.mm(x, y) # dot product (mm=matrix multiplication)\n",
    "\n",
    "# Time CPU\n",
    "print('CPU: {}ms'.format(timeit(square, number=number)*1000))\n",
    "# Time GPU\n",
    "x, y = x.cuda(), y.cuda()\n",
    "print('GPU: {}ms'.format(timeit(square, number=number)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors provide automatic differentiation.\n",
    "\n",
    "As you might know, previous versions of Pytorch used Variables, which were wrappers around tensors for differentiation. Starting with pytorch 0.4.0, this wrapping is done internally in the Tensor class and you can, and should, differentiate Tensors directly. However, it is possible that you walk on references to Variables, e.g. in your error messages.\n",
    "\n",
    "What you need to remember :\n",
    "\n",
    "- Tensors you are differentiating with respect to must have `requires_grad=True`\n",
    "- Call `.backward()` on scalar variables you are differentiating\n",
    "- To differentiate a vector, sum it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.])\n",
      "tensor([ 0.,  1.,  4.,  9.])\n",
      "tensor([ 0.,  2.,  4.,  6.])\n"
     ]
    }
   ],
   "source": [
    "# Create differentiable tensor\n",
    "x = torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "# Calculate y=sum(x**2)\n",
    "y = x**2\n",
    "# Calculate gradient (dy/dx=2x)\n",
    "y.sum().backward()\n",
    "# Print values\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiation accumulates gradients. This is sometimes what you want and sometimes not. **Make sure to zero gradients between batches if performing gradient descent or you will get strange results!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  2.,  4.,  6.])\n",
      "tensor([  0.,   4.,   8.,  12.])\n",
      "tensor([ 0.,  2.,  4.,  6.])\n"
     ]
    }
   ],
   "source": [
    "# Create a variable\n",
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "# Differentiate\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Differentiate again (accumulates gradient)\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)\n",
    "# Zero gradient before differentiating\n",
    "x.grad.data.zero_()\n",
    "torch.sum(x**2).backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a Tensor with gradient cannot be exported to numpy directly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8c8213f31fb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# raises an exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "x.numpy() # raises an exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that pytorch remembers the graph of all computations to perform differenciation. To be integrated to this graph the raw data is wrapped internally to the Tensor class (like what was formerly a Variable). You can detach the tensor from the graph using the **.detach()** method, which returns a tensor with the same data but requires_grad set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1., 16., 81.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(torch.arange(0,4), requires_grad=True)\n",
    "y=x**2\n",
    "z=y**2\n",
    "z.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another reason to use this method is that updating the graph can use a lot of memory. If you are in a context where you have a differentiable tensor that you don't need to differentiate, think of detaching it from the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch provides a framework for developing neural network modules. They take care of many things, the main one being wrapping and tracking a list of parameters for you.\n",
    "You have several ways of building and using a network, offering different tradeoffs between freedom and simplicity.\n",
    "\n",
    "torch.nn provides basic 1-layer nets, such as Linear (perceptron) and activation layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11.5822,  -1.9862,  14.1805,   5.3723,   1.9367,  -5.8406,\n",
      "        -11.5825,  14.0956,  12.5262,  -7.0499])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,32)\n",
    "net = torch.nn.Linear(32,10)\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All nn.Module objects are reusable as components of bigger networks ! That is how you build personnalized nets. The simplest way is to use the nn.Sequential class.\n",
    "\n",
    "You can also create your own class that inherits n.Module. The forward method should precise what happens in the forward pass given an input. This enables you to precise behaviors more complicated than just applying layers one after another, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple sequential network (`nn.Module` object) from layers (other `nn.Module` objects).\n",
    "# Here a MLP with 2 layers and sigmoid activation.\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(32,128),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(128,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a more customizable network module (equivalent here)\n",
    "class MyNetwork(torch.nn.Module):\n",
    "    # you can use the layer sizes as initialization arguments if you want to\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size,hidden_size)\n",
    "        self.layer2 = torch.nn.Sigmoid()\n",
    "        self.layer3 = torch.nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, input_val):\n",
    "        h = input_val\n",
    "        h = self.layer1(h)\n",
    "        h = self.layer2(h)\n",
    "        h = self.layer3(h)\n",
    "        return h\n",
    "\n",
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network tracks parameters, and you can access them through the **parameters()** method, which returns a python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0914, -0.1410, -0.1612,  ...,  0.0039, -0.1229, -0.0204],\n",
      "        [ 0.0104, -0.0732,  0.0486,  ...,  0.0797,  0.0237, -0.1745],\n",
      "        [ 0.0532,  0.1423, -0.0205,  ..., -0.1413,  0.0120, -0.1228],\n",
      "        ...,\n",
      "        [-0.0851, -0.0668,  0.0695,  ...,  0.1764,  0.0027, -0.0931],\n",
      "        [-0.1475, -0.0176, -0.1509,  ...,  0.0139, -0.0923, -0.1109],\n",
      "        [-0.1513, -0.1019, -0.1299,  ..., -0.1663, -0.0874,  0.0098]])\n",
      "Parameter containing:\n",
      "tensor([-0.1361,  0.1213, -0.1093, -0.1311, -0.1273,  0.1518, -0.0988,\n",
      "        -0.0905, -0.1317,  0.0209,  0.0389,  0.1195,  0.1663, -0.0897,\n",
      "        -0.1556,  0.1120,  0.1077,  0.1760,  0.1691, -0.0580, -0.0145,\n",
      "        -0.1578,  0.0175, -0.0486, -0.1148, -0.0834, -0.0391,  0.0975,\n",
      "        -0.0538,  0.0197,  0.0966,  0.1327, -0.0276,  0.0549, -0.0014,\n",
      "         0.0823, -0.0015, -0.0788, -0.0588,  0.0901, -0.0841,  0.0586,\n",
      "         0.0262,  0.1208,  0.0515, -0.0637, -0.0205,  0.1027,  0.0601,\n",
      "         0.0068, -0.0869,  0.1384, -0.1525, -0.1604,  0.0391,  0.1156,\n",
      "        -0.1668,  0.0624, -0.0872,  0.1238, -0.0450,  0.1657, -0.0723,\n",
      "        -0.0086,  0.1165, -0.0475,  0.0095,  0.1352, -0.0624, -0.1391,\n",
      "         0.0154, -0.1571,  0.1266,  0.0170, -0.0480, -0.0265,  0.1470,\n",
      "        -0.0524, -0.0017,  0.0252,  0.0165, -0.0918, -0.0113, -0.0718,\n",
      "        -0.0151, -0.1260,  0.0642, -0.0713,  0.0245,  0.1725,  0.0499,\n",
      "        -0.1110, -0.1376, -0.1105,  0.1207,  0.1387,  0.0600, -0.1374,\n",
      "         0.0476,  0.0590, -0.0333,  0.0529,  0.1531, -0.0427,  0.1117,\n",
      "         0.0414, -0.1569, -0.0139, -0.0453, -0.0168, -0.0082, -0.1323,\n",
      "         0.0867, -0.1176, -0.0870,  0.1705,  0.0376, -0.0153,  0.0886,\n",
      "         0.1721, -0.0890, -0.1637, -0.0261, -0.1107, -0.1660, -0.0596,\n",
      "        -0.0415, -0.0018])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 3.9634,  5.9595, -2.1337,  ..., -7.9369,  2.3594,  0.5043],\n",
      "        [-6.9452, -8.5407, -1.3389,  ...,  1.1464,  3.4605, -6.2085],\n",
      "        [-8.5986, -3.2125,  8.8029,  ..., -2.3267, -3.7548, -6.5622],\n",
      "        ...,\n",
      "        [-2.9621, -3.0379,  8.2476,  ..., -7.9438, -7.0208, -8.3517],\n",
      "        [ 4.1355,  1.2015, -1.2601,  ...,  7.6290,  7.5596, -0.1385],\n",
      "        [ 4.8073, -7.2663,  2.8036,  ..., -0.1416, -1.7610, -1.3560]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [ 0.2555, -3.2014, -0.4812,  1.6570,  5.3790, -4.2366, -1.9174,\n",
      "         5.2931, -1.9425, -1.5647])\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are of type Parameter, which is basically a wrapper for a tensor. How does pytorch retrieve your network's parameters ? They are simply all the attributes of type Parameter in your network. Moreover, if an attribute is of type nn.Module, its own parameters are added to your network's parameters ! This is why, when you define a network by adding up basic components such as nn.Linear, you should never have to explicitely define parameters.\n",
    "\n",
    "However, if you are in a case where no pytorch default module does what you need, you can define parameters explicitely (this should be rare). For the record, let's build the previous MLP with personnalized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetworkWithParams(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(MyNetworkWithParams,self).__init__()\n",
    "        self.layer1_weights = nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.layer1_bias = nn.Parameter(torch.randn(hidden_size))\n",
    "        self.layer2_weights = nn.Parameter(torch.randn(hidden_size,output_size))\n",
    "        self.layer2_bias = nn.Parameter(torch.randn(output_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        h1 = torch.matmul(x,self.layer1_weights) + self.layer1_bias\n",
    "        h1_act = torch.max(h1, torch.zeros(h1.size())) # ReLU\n",
    "        output = torch.matmul(h1_act,self.layer2_weights) + self.layer2_bias\n",
    "        return output\n",
    "\n",
    "net = MyNetworkWithParams(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters are useful in that they are meant to be all the network's weights that will be optimized during training. If you were needing to use a tensor in your computational graph that you want to remain constant, just define it as a regular tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyNetwork(32,128,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nn.Module also provides loss functions, such as cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3035)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([np.arange(32), np.zeros(32),np.ones(32)]).float()\n",
    "y = torch.tensor([0,3,9])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.CrossEntropyLoss does both the softmax and the actual cross-entropy : given $output$ of size $(n,d)$ and $y$ of size $n$ and values in $0,1,...,d-1$, it computes $\\sum_{i=0}^{n-1}log(s[i,y[i]])$ where $s[i,j] = \\frac{e^{output[i,j]}}{\\sum_{j'=0}^{d-1}e^{output[i,j']}}$\n",
    "\n",
    "You can also compose nn.LogSoftmax and nn.NLLLoss to get the same result. Note that all these use the log-softmax rather than the softmax, for stability in the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dl/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3035)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equivalent\n",
    "criterion2 = nn.NLLLoss()\n",
    "sf = nn.LogSoftmax()\n",
    "output = net(x)\n",
    "loss = criterion(sf(output),y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to perform the backward pass, just execute **loss.backward()** ! It will update gradients in all differentiable tensors in the graph, which in particular includes all the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9256e-03,  7.6095e-03,  7.2935e-03,  ..., -1.2395e-03,\n",
      "         -1.5556e-03, -1.8716e-03],\n",
      "        [ 6.0576e-03,  6.0517e-03,  6.0459e-03,  ...,  5.8876e-03,\n",
      "          5.8817e-03,  5.8759e-03],\n",
      "        [ 4.6112e-03,  2.8569e-03,  1.1025e-03,  ..., -4.6265e-02,\n",
      "         -4.8019e-02, -4.9773e-02],\n",
      "        ...,\n",
      "        [-5.4263e-03, -5.4263e-03, -5.4263e-03,  ..., -5.4264e-03,\n",
      "         -5.4264e-03, -5.4264e-03],\n",
      "        [ 5.0389e-03,  5.2486e-03,  5.4582e-03,  ...,  1.1120e-02,\n",
      "          1.1329e-02,  1.1539e-02],\n",
      "        [ 1.5522e-03,  1.5693e-03,  1.5865e-03,  ...,  2.0494e-03,\n",
      "          2.0666e-03,  2.0837e-03]])\n",
      "tensor(1.00000e-02 *\n",
      "       [ 0.7206,  0.2780,  0.4303,  0.2088, -0.5509, -0.4945,  0.3033,\n",
      "         0.5389, -0.5690,  0.0489,  0.5832, -0.2049, -0.1358, -0.1814,\n",
      "         0.6309,  0.1223,  0.1294,  0.2739,  0.0308,  0.0416,  0.2922,\n",
      "         0.7409, -0.8522, -1.0807, -0.0079,  0.5953,  0.2272,  0.5640,\n",
      "        -0.1492, -0.7269, -0.5388, -0.3805,  0.8224,  0.4618,  0.4251,\n",
      "        -0.0609, -0.2262, -0.6851,  0.0111,  0.3850,  0.3200,  0.8511,\n",
      "         0.0027, -0.3369, -0.4314, -0.4417,  0.1802, -0.7477, -0.0071,\n",
      "         0.0724, -0.1050, -0.3081, -0.1010, -0.3550, -0.5084,  0.2086,\n",
      "        -0.5358,  0.3933, -0.5708,  0.6434, -0.7972, -0.3309, -0.1035,\n",
      "         0.4113,  0.9497,  0.0169,  0.0861, -0.3526,  0.1175, -0.4505,\n",
      "        -0.8279,  0.7344,  0.5125, -0.6260, -0.5065,  0.0285,  0.2366,\n",
      "        -0.2201, -0.2111,  0.0544, -0.6667,  0.2205,  0.8400, -0.1969,\n",
      "        -0.6820, -0.5893, -0.0692,  1.0325,  0.1592,  0.0310, -0.1203,\n",
      "         0.7506, -0.2161,  0.9155,  0.6018,  0.2355,  0.2005, -0.6417,\n",
      "         0.0798,  0.2531,  0.5190,  0.1294,  0.7138, -0.0151,  0.1894,\n",
      "        -0.1113, -0.2628,  0.2980,  0.1718, -0.0284,  0.7508,  0.6224,\n",
      "        -0.2653, -0.3655,  0.1605, -0.0856,  0.4647,  1.1092, -0.3445,\n",
      "         0.4274,  0.1762,  0.0406, -0.4334, -0.0056,  0.0177, -0.4382,\n",
      "        -0.1462,  0.7120])\n",
      "tensor([[-0.2380, -0.2353,  0.0158,  ..., -0.2356,  0.0339, -0.2359],\n",
      "        [ 0.0575,  0.0651,  0.0330,  ...,  0.0649,  0.0290,  0.0644],\n",
      "        [ 0.0551,  0.0622,  0.0307,  ...,  0.0621,  0.0268,  0.0616],\n",
      "        ...,\n",
      "        [ 0.0525,  0.0585,  0.0256,  ...,  0.0584,  0.0219,  0.0580],\n",
      "        [ 0.0532,  0.0602,  0.0316,  ...,  0.0600,  0.0280,  0.0595],\n",
      "        [-0.1190, -0.1816, -0.1212,  ..., -0.1885, -0.1072, -0.1868]])\n",
      "tensor([-0.2018,  0.0918,  0.0870, -0.2484,  0.0717,  0.2301,  0.0699,\n",
      "         0.0786,  0.0862, -0.2649])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "# Check that the parameters now have gradients\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0159,  0.0152,  0.0146,  ..., -0.0025, -0.0031, -0.0037],\n",
      "        [ 0.0121,  0.0121,  0.0121,  ...,  0.0118,  0.0118,  0.0118],\n",
      "        [ 0.0092,  0.0057,  0.0022,  ..., -0.0925, -0.0960, -0.0995],\n",
      "        ...,\n",
      "        [-0.0109, -0.0109, -0.0109,  ..., -0.0109, -0.0109, -0.0109],\n",
      "        [ 0.0101,  0.0105,  0.0109,  ...,  0.0222,  0.0227,  0.0231],\n",
      "        [ 0.0031,  0.0031,  0.0032,  ...,  0.0041,  0.0041,  0.0042]])\n",
      "tensor(1.00000e-02 *\n",
      "       [ 1.4413,  0.5560,  0.8606,  0.4177, -1.1018, -0.9890,  0.6065,\n",
      "         1.0778, -1.1381,  0.0977,  1.1665, -0.4099, -0.2717, -0.3628,\n",
      "         1.2618,  0.2446,  0.2588,  0.5479,  0.0616,  0.0832,  0.5843,\n",
      "         1.4819, -1.7043, -2.1615, -0.0159,  1.1907,  0.4545,  1.1279,\n",
      "        -0.2985, -1.4539, -1.0777, -0.7609,  1.6447,  0.9236,  0.8502,\n",
      "        -0.1219, -0.4525, -1.3703,  0.0223,  0.7700,  0.6401,  1.7022,\n",
      "         0.0055, -0.6738, -0.8629, -0.8833,  0.3604, -1.4954, -0.0141,\n",
      "         0.1449, -0.2100, -0.6163, -0.2020, -0.7101, -1.0167,  0.4171,\n",
      "        -1.0715,  0.7865, -1.1416,  1.2867, -1.5945, -0.6618, -0.2070,\n",
      "         0.8227,  1.8994,  0.0339,  0.1722, -0.7051,  0.2351, -0.9010,\n",
      "        -1.6558,  1.4689,  1.0251, -1.2520, -1.0131,  0.0569,  0.4731,\n",
      "        -0.4401, -0.4222,  0.1087, -1.3334,  0.4410,  1.6799, -0.3937,\n",
      "        -1.3640, -1.1785, -0.1384,  2.0650,  0.3184,  0.0620, -0.2405,\n",
      "         1.5012, -0.4322,  1.8309,  1.2036,  0.4711,  0.4010, -1.2833,\n",
      "         0.1597,  0.5062,  1.0379,  0.2587,  1.4277, -0.0302,  0.3788,\n",
      "        -0.2225, -0.5256,  0.5959,  0.3437, -0.0567,  1.5015,  1.2448,\n",
      "        -0.5306, -0.7310,  0.3210, -0.1712,  0.9293,  2.2184, -0.6889,\n",
      "         0.8548,  0.3523,  0.0812, -0.8668, -0.0112,  0.0355, -0.8765,\n",
      "        -0.2924,  1.4241])\n",
      "tensor([[-0.4761, -0.4706,  0.0316,  ..., -0.4712,  0.0678, -0.4718],\n",
      "        [ 0.1151,  0.1302,  0.0660,  ...,  0.1299,  0.0579,  0.1288],\n",
      "        [ 0.1102,  0.1245,  0.0614,  ...,  0.1242,  0.0537,  0.1232],\n",
      "        ...,\n",
      "        [ 0.1050,  0.1170,  0.0512,  ...,  0.1168,  0.0437,  0.1160],\n",
      "        [ 0.1065,  0.1204,  0.0633,  ...,  0.1200,  0.0559,  0.1190],\n",
      "        [-0.2380, -0.3631, -0.2423,  ..., -0.3771, -0.2144, -0.3736]])\n",
      "tensor([-0.4035,  0.1837,  0.1739, -0.4969,  0.1433,  0.4602,  0.1398,\n",
      "         0.1571,  0.1723, -0.5299])\n",
      "tensor([[ 7.9256e-03,  7.6095e-03,  7.2935e-03,  ..., -1.2395e-03,\n",
      "         -1.5556e-03, -1.8716e-03],\n",
      "        [ 6.0576e-03,  6.0517e-03,  6.0459e-03,  ...,  5.8876e-03,\n",
      "          5.8817e-03,  5.8759e-03],\n",
      "        [ 4.6112e-03,  2.8569e-03,  1.1025e-03,  ..., -4.6265e-02,\n",
      "         -4.8019e-02, -4.9773e-02],\n",
      "        ...,\n",
      "        [-5.4263e-03, -5.4263e-03, -5.4263e-03,  ..., -5.4264e-03,\n",
      "         -5.4264e-03, -5.4264e-03],\n",
      "        [ 5.0389e-03,  5.2486e-03,  5.4582e-03,  ...,  1.1120e-02,\n",
      "          1.1329e-02,  1.1539e-02],\n",
      "        [ 1.5522e-03,  1.5693e-03,  1.5865e-03,  ...,  2.0494e-03,\n",
      "          2.0666e-03,  2.0837e-03]])\n",
      "tensor(1.00000e-02 *\n",
      "       [ 0.7206,  0.2780,  0.4303,  0.2088, -0.5509, -0.4945,  0.3033,\n",
      "         0.5389, -0.5690,  0.0489,  0.5832, -0.2049, -0.1358, -0.1814,\n",
      "         0.6309,  0.1223,  0.1294,  0.2739,  0.0308,  0.0416,  0.2922,\n",
      "         0.7409, -0.8522, -1.0807, -0.0079,  0.5953,  0.2272,  0.5640,\n",
      "        -0.1492, -0.7269, -0.5388, -0.3805,  0.8224,  0.4618,  0.4251,\n",
      "        -0.0609, -0.2262, -0.6851,  0.0111,  0.3850,  0.3200,  0.8511,\n",
      "         0.0027, -0.3369, -0.4314, -0.4417,  0.1802, -0.7477, -0.0071,\n",
      "         0.0724, -0.1050, -0.3081, -0.1010, -0.3550, -0.5084,  0.2086,\n",
      "        -0.5358,  0.3933, -0.5708,  0.6434, -0.7972, -0.3309, -0.1035,\n",
      "         0.4113,  0.9497,  0.0169,  0.0861, -0.3526,  0.1175, -0.4505,\n",
      "        -0.8279,  0.7344,  0.5125, -0.6260, -0.5065,  0.0285,  0.2366,\n",
      "        -0.2201, -0.2111,  0.0544, -0.6667,  0.2205,  0.8400, -0.1969,\n",
      "        -0.6820, -0.5893, -0.0692,  1.0325,  0.1592,  0.0310, -0.1203,\n",
      "         0.7506, -0.2161,  0.9155,  0.6018,  0.2355,  0.2005, -0.6417,\n",
      "         0.0798,  0.2531,  0.5190,  0.1294,  0.7138, -0.0151,  0.1894,\n",
      "        -0.1113, -0.2628,  0.2980,  0.1718, -0.0284,  0.7508,  0.6224,\n",
      "        -0.2653, -0.3655,  0.1605, -0.0856,  0.4647,  1.1092, -0.3445,\n",
      "         0.4274,  0.1762,  0.0406, -0.4334, -0.0056,  0.0177, -0.4382,\n",
      "        -0.1462,  0.7120])\n",
      "tensor([[-0.2380, -0.2353,  0.0158,  ..., -0.2356,  0.0339, -0.2359],\n",
      "        [ 0.0575,  0.0651,  0.0330,  ...,  0.0649,  0.0290,  0.0644],\n",
      "        [ 0.0551,  0.0622,  0.0307,  ...,  0.0621,  0.0268,  0.0616],\n",
      "        ...,\n",
      "        [ 0.0525,  0.0585,  0.0256,  ...,  0.0584,  0.0219,  0.0580],\n",
      "        [ 0.0532,  0.0602,  0.0316,  ...,  0.0600,  0.0280,  0.0595],\n",
      "        [-0.1190, -0.1816, -0.1212,  ..., -0.1885, -0.1072, -0.1868]])\n",
      "tensor([-0.2018,  0.0918,  0.0870, -0.2484,  0.0717,  0.2301,  0.0699,\n",
      "         0.0786,  0.0862, -0.2649])\n"
     ]
    }
   ],
   "source": [
    "# if I forward prop and backward prop again, gradients accumulate :\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)\n",
    "\n",
    "# you can remove this behavior by reinitializing the gradients in your network parameters :\n",
    "net.zero_grad()\n",
    "output = net(x)\n",
    "loss = criterion(output,y)\n",
    "loss.backward()\n",
    "for param in net.parameters():\n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did backpropagation, but still didn't perform gradient descent. Let's define an optimizer on the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters before gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[-0.1337,  0.1232,  0.0619,  ...,  0.0630,  0.0150,  0.0703],\n",
      "        [ 0.1393,  0.1379,  0.0322,  ..., -0.1695, -0.0602,  0.0817],\n",
      "        [-0.0893, -0.0196,  0.0742,  ..., -0.0124,  0.1540, -0.1503],\n",
      "        ...,\n",
      "        [-0.1230, -0.0295, -0.0187,  ...,  0.1406, -0.0945, -0.1394],\n",
      "        [ 0.0768, -0.1488, -0.0586,  ..., -0.1046, -0.0873,  0.0184],\n",
      "        [ 0.1067,  0.1486,  0.1531,  ..., -0.1580,  0.0494,  0.1125]])\n",
      "Parameter containing:\n",
      "tensor([-0.0457, -0.0032,  0.0048, -0.1488,  0.0679,  0.0036, -0.1065,\n",
      "         0.1504,  0.0788, -0.0569, -0.0598, -0.1160, -0.0674, -0.0856,\n",
      "        -0.1280, -0.0335, -0.1202, -0.0329,  0.1357, -0.0265,  0.0350,\n",
      "         0.1000, -0.0399,  0.0109, -0.1117, -0.0145,  0.0957,  0.0403,\n",
      "         0.0655,  0.0713,  0.1729,  0.1131, -0.1663,  0.0978,  0.0194,\n",
      "         0.0700,  0.0766, -0.0120,  0.1037, -0.0971, -0.1386,  0.0835,\n",
      "         0.1103, -0.0894,  0.0634, -0.1210, -0.0439, -0.0426,  0.0150,\n",
      "         0.0469, -0.1509,  0.1543, -0.1168,  0.0468, -0.0130,  0.0367,\n",
      "        -0.0159,  0.0763,  0.0937,  0.1605,  0.1382, -0.1014,  0.1634,\n",
      "        -0.0581, -0.0133, -0.1343,  0.0046, -0.1116,  0.0557, -0.1463,\n",
      "         0.0293, -0.1381,  0.0322, -0.0852, -0.0037,  0.0348,  0.1497,\n",
      "        -0.0885, -0.1102, -0.0753, -0.1480, -0.1525, -0.1518,  0.1652,\n",
      "         0.1715, -0.0142,  0.0242,  0.0294, -0.0644,  0.1225, -0.0761,\n",
      "         0.0665, -0.0373,  0.1049, -0.0237, -0.0024,  0.1669, -0.0687,\n",
      "         0.1318,  0.0016, -0.0567,  0.0794, -0.0989, -0.1279,  0.1430,\n",
      "        -0.0992,  0.0060,  0.1300, -0.1613,  0.0046,  0.0081, -0.0860,\n",
      "         0.0264,  0.1033, -0.0697, -0.1009, -0.1530,  0.0561,  0.0417,\n",
      "        -0.0733, -0.0072,  0.1730, -0.0995, -0.0341, -0.0358, -0.1038,\n",
      "        -0.0516, -0.1388])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 8.3587,  8.3019,  8.7071,  ...,  7.0374, -6.3483, -2.2711],\n",
      "        [ 8.6435,  7.9081,  5.7254,  ..., -3.6644,  3.9103,  6.3753],\n",
      "        [-3.6856, -0.6393, -2.9268,  ..., -5.8084, -3.2163,  8.7503],\n",
      "        ...,\n",
      "        [-0.9020,  5.9406,  3.8662,  ..., -3.9140,  2.5277, -3.5947],\n",
      "        [-2.2811,  1.5488,  5.4094,  ..., -8.7021, -1.9296, -4.2304],\n",
      "        [-5.8544, -3.6339, -3.9102,  ...,  6.3260, -5.6428, -0.6259]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [-0.7918,  4.6214,  1.8393, -1.8574, -4.4270,  7.8209,  7.8459,\n",
      "         5.1486,  1.8408,  0.9547])\n",
      "Parameters after gradient descent :\n",
      "Parameter containing:\n",
      "tensor([[-0.1338,  0.1231,  0.0618,  ...,  0.0630,  0.0151,  0.0703],\n",
      "        [ 0.1392,  0.1378,  0.0321,  ..., -0.1695, -0.0603,  0.0817],\n",
      "        [-0.0893, -0.0196,  0.0742,  ..., -0.0120,  0.1545, -0.1498],\n",
      "        ...,\n",
      "        [-0.1229, -0.0294, -0.0186,  ...,  0.1407, -0.0944, -0.1393],\n",
      "        [ 0.0768, -0.1489, -0.0587,  ..., -0.1047, -0.0874,  0.0183],\n",
      "        [ 0.1067,  0.1486,  0.1531,  ..., -0.1580,  0.0493,  0.1125]])\n",
      "Parameter containing:\n",
      "tensor([-0.0458, -0.0033,  0.0048, -0.1488,  0.0679,  0.0037, -0.1065,\n",
      "         0.1503,  0.0788, -0.0569, -0.0598, -0.1160, -0.0674, -0.0855,\n",
      "        -0.1281, -0.0335, -0.1202, -0.0329,  0.1357, -0.0265,  0.0350,\n",
      "         0.0999, -0.0399,  0.0110, -0.1117, -0.0145,  0.0957,  0.0402,\n",
      "         0.0655,  0.0714,  0.1729,  0.1132, -0.1664,  0.0978,  0.0194,\n",
      "         0.0700,  0.0766, -0.0119,  0.1037, -0.0971, -0.1386,  0.0834,\n",
      "         0.1103, -0.0894,  0.0634, -0.1209, -0.0439, -0.0425,  0.0150,\n",
      "         0.0469, -0.1509,  0.1543, -0.1168,  0.0468, -0.0129,  0.0367,\n",
      "        -0.0159,  0.0763,  0.0938,  0.1605,  0.1383, -0.1014,  0.1634,\n",
      "        -0.0581, -0.0134, -0.1343,  0.0046, -0.1115,  0.0557, -0.1463,\n",
      "         0.0294, -0.1382,  0.0321, -0.0852, -0.0037,  0.0348,  0.1497,\n",
      "        -0.0885, -0.1101, -0.0753, -0.1479, -0.1525, -0.1519,  0.1652,\n",
      "         0.1716, -0.0141,  0.0242,  0.0293, -0.0645,  0.1225, -0.0761,\n",
      "         0.0665, -0.0373,  0.1048, -0.0238, -0.0025,  0.1669, -0.0686,\n",
      "         0.1317,  0.0016, -0.0568,  0.0794, -0.0989, -0.1279,  0.1430,\n",
      "        -0.0992,  0.0061,  0.1300, -0.1613,  0.0046,  0.0081, -0.0860,\n",
      "         0.0264,  0.1033, -0.0697, -0.1009, -0.1530,  0.0560,  0.0417,\n",
      "        -0.0733, -0.0072,  0.1730, -0.0995, -0.0341, -0.0358, -0.1038,\n",
      "        -0.0516, -0.1389])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 8.5967,  8.5372,  8.6913,  ...,  7.2730, -6.3822, -2.0352],\n",
      "        [ 8.5860,  7.8430,  5.6924,  ..., -3.7294,  3.8813,  6.3109],\n",
      "        [-3.7407, -0.7015, -2.9576,  ..., -5.8705, -3.2432,  8.6887],\n",
      "        ...,\n",
      "        [-0.9545,  5.8821,  3.8406,  ..., -3.9724,  2.5059, -3.6526],\n",
      "        [-2.3343,  1.4885,  5.3778,  ..., -8.7621, -1.9575, -4.2899],\n",
      "        [-5.7354, -3.4524, -3.7891,  ...,  6.5146, -5.5356, -0.4391]])\n",
      "Parameter containing:\n",
      "tensor(1.00000e-02 *\n",
      "       [-0.5900,  4.5296,  1.7523, -1.6090, -4.4987,  7.5908,  7.7760,\n",
      "         5.0700,  1.7547,  1.2197])\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Parameters before gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Parameters after gradient descent :\")\n",
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1645)\n",
      "tensor(2.0416)\n",
      "tensor(1.9329)\n",
      "tensor(1.8363)\n",
      "tensor(1.7503)\n",
      "tensor(1.6735)\n",
      "tensor(1.6045)\n",
      "tensor(1.5419)\n",
      "tensor(1.4850)\n",
      "tensor(1.4329)\n",
      "tensor(1.3851)\n",
      "tensor(1.3412)\n",
      "tensor(1.3008)\n",
      "tensor(1.2635)\n",
      "tensor(1.2288)\n",
      "tensor(1.1966)\n",
      "tensor(1.1666)\n",
      "tensor(1.1386)\n",
      "tensor(1.1123)\n",
      "tensor(1.0877)\n",
      "tensor(1.0646)\n",
      "tensor(1.0428)\n",
      "tensor(1.0222)\n",
      "tensor(1.0028)\n",
      "tensor(0.9845)\n",
      "tensor(0.9671)\n",
      "tensor(0.9506)\n",
      "tensor(0.9349)\n",
      "tensor(0.9200)\n",
      "tensor(0.9057)\n",
      "tensor(0.8922)\n",
      "tensor(0.8792)\n",
      "tensor(0.8668)\n",
      "tensor(0.8550)\n",
      "tensor(0.8436)\n",
      "tensor(0.8328)\n",
      "tensor(0.8223)\n",
      "tensor(0.8123)\n",
      "tensor(0.8026)\n",
      "tensor(0.7933)\n",
      "tensor(0.7843)\n",
      "tensor(0.7757)\n",
      "tensor(0.7673)\n",
      "tensor(0.7593)\n",
      "tensor(0.7515)\n",
      "tensor(0.7439)\n",
      "tensor(0.7366)\n",
      "tensor(0.7295)\n",
      "tensor(0.7227)\n",
      "tensor(0.7160)\n",
      "tensor(0.7095)\n",
      "tensor(0.7032)\n",
      "tensor(0.6971)\n",
      "tensor(0.6911)\n",
      "tensor(0.6853)\n",
      "tensor(0.6796)\n",
      "tensor(0.6741)\n",
      "tensor(0.6687)\n",
      "tensor(0.6635)\n",
      "tensor(0.6584)\n",
      "tensor(0.6534)\n",
      "tensor(0.6485)\n",
      "tensor(0.6437)\n",
      "tensor(0.6390)\n",
      "tensor(0.6344)\n",
      "tensor(0.6300)\n",
      "tensor(0.6256)\n",
      "tensor(0.6213)\n",
      "tensor(0.6171)\n",
      "tensor(0.6129)\n",
      "tensor(0.6089)\n",
      "tensor(0.6049)\n",
      "tensor(0.6010)\n",
      "tensor(0.5972)\n",
      "tensor(0.5934)\n",
      "tensor(0.5897)\n",
      "tensor(0.5861)\n",
      "tensor(0.5825)\n",
      "tensor(0.5790)\n",
      "tensor(0.5756)\n",
      "tensor(0.5722)\n",
      "tensor(0.5689)\n",
      "tensor(0.5656)\n",
      "tensor(0.5624)\n",
      "tensor(0.5592)\n",
      "tensor(0.5561)\n",
      "tensor(0.5530)\n",
      "tensor(0.5499)\n",
      "tensor(0.5469)\n",
      "tensor(0.5440)\n",
      "tensor(0.5411)\n",
      "tensor(0.5382)\n",
      "tensor(0.5354)\n",
      "tensor(0.5326)\n",
      "tensor(0.5298)\n",
      "tensor(0.5271)\n",
      "tensor(0.5244)\n",
      "tensor(0.5218)\n",
      "tensor(0.5192)\n",
      "tensor(0.5166)\n",
      "tensor(0.5140)\n",
      "tensor(0.5115)\n",
      "tensor(0.5090)\n",
      "tensor(0.5066)\n",
      "tensor(0.5041)\n",
      "tensor(0.5017)\n",
      "tensor(0.4994)\n",
      "tensor(0.4970)\n",
      "tensor(0.4947)\n",
      "tensor(0.4924)\n",
      "tensor(0.4901)\n",
      "tensor(0.4879)\n",
      "tensor(0.4856)\n",
      "tensor(0.4834)\n",
      "tensor(0.4813)\n",
      "tensor(0.4791)\n",
      "tensor(0.4770)\n",
      "tensor(0.4749)\n",
      "tensor(0.4728)\n",
      "tensor(0.4707)\n",
      "tensor(0.4686)\n",
      "tensor(0.4666)\n",
      "tensor(0.4646)\n",
      "tensor(0.4626)\n",
      "tensor(0.4606)\n",
      "tensor(0.4587)\n",
      "tensor(0.4567)\n",
      "tensor(0.4548)\n",
      "tensor(0.4529)\n",
      "tensor(0.4510)\n",
      "tensor(0.4491)\n",
      "tensor(0.4473)\n",
      "tensor(0.4454)\n",
      "tensor(0.4436)\n",
      "tensor(0.4418)\n",
      "tensor(0.4400)\n",
      "tensor(0.4382)\n",
      "tensor(0.4364)\n",
      "tensor(0.4347)\n",
      "tensor(0.4330)\n",
      "tensor(0.4312)\n",
      "tensor(0.4295)\n",
      "tensor(0.4278)\n",
      "tensor(0.4261)\n",
      "tensor(0.4245)\n",
      "tensor(0.4228)\n",
      "tensor(0.4212)\n",
      "tensor(0.4195)\n",
      "tensor(0.4179)\n",
      "tensor(0.4163)\n",
      "tensor(0.4147)\n",
      "tensor(0.4131)\n",
      "tensor(0.4116)\n",
      "tensor(0.4100)\n",
      "tensor(0.4085)\n",
      "tensor(0.4069)\n",
      "tensor(0.4054)\n",
      "tensor(0.4039)\n",
      "tensor(0.4024)\n",
      "tensor(0.4009)\n",
      "tensor(0.3994)\n",
      "tensor(0.3979)\n",
      "tensor(0.3965)\n",
      "tensor(0.3950)\n",
      "tensor(0.3936)\n",
      "tensor(0.3921)\n",
      "tensor(0.3907)\n",
      "tensor(0.3893)\n",
      "tensor(0.3879)\n",
      "tensor(0.3865)\n",
      "tensor(0.3851)\n",
      "tensor(0.3837)\n",
      "tensor(0.3824)\n",
      "tensor(0.3810)\n",
      "tensor(0.3796)\n",
      "tensor(0.3783)\n",
      "tensor(0.3770)\n",
      "tensor(0.3756)\n",
      "tensor(0.3743)\n",
      "tensor(0.3730)\n",
      "tensor(0.3717)\n",
      "tensor(0.3704)\n",
      "tensor(0.3691)\n",
      "tensor(0.3679)\n",
      "tensor(0.3666)\n",
      "tensor(0.3653)\n",
      "tensor(0.3641)\n",
      "tensor(0.3628)\n",
      "tensor(0.3616)\n",
      "tensor(0.3603)\n",
      "tensor(0.3591)\n",
      "tensor(0.3579)\n",
      "tensor(0.3567)\n",
      "tensor(0.3555)\n",
      "tensor(0.3543)\n",
      "tensor(0.3531)\n",
      "tensor(0.3519)\n",
      "tensor(0.3507)\n",
      "tensor(0.3495)\n",
      "tensor(0.3484)\n",
      "tensor(0.3472)\n",
      "tensor(0.3461)\n",
      "tensor(0.3449)\n",
      "tensor(0.3438)\n",
      "tensor(0.3426)\n",
      "tensor(0.3415)\n",
      "tensor(0.3404)\n",
      "tensor(0.3393)\n",
      "tensor(0.3382)\n",
      "tensor(0.3371)\n",
      "tensor(0.3360)\n",
      "tensor(0.3349)\n",
      "tensor(0.3338)\n",
      "tensor(0.3327)\n",
      "tensor(0.3316)\n",
      "tensor(0.3306)\n",
      "tensor(0.3295)\n",
      "tensor(0.3284)\n",
      "tensor(0.3274)\n",
      "tensor(0.3263)\n",
      "tensor(0.3253)\n",
      "tensor(0.3242)\n",
      "tensor(0.3232)\n",
      "tensor(0.3222)\n",
      "tensor(0.3212)\n",
      "tensor(0.3202)\n",
      "tensor(0.3191)\n",
      "tensor(0.3181)\n",
      "tensor(0.3171)\n",
      "tensor(0.3161)\n",
      "tensor(0.3151)\n",
      "tensor(0.3142)\n",
      "tensor(0.3132)\n",
      "tensor(0.3122)\n",
      "tensor(0.3112)\n",
      "tensor(0.3103)\n",
      "tensor(0.3093)\n",
      "tensor(0.3083)\n",
      "tensor(0.3074)\n",
      "tensor(0.3064)\n",
      "tensor(0.3055)\n",
      "tensor(0.3045)\n",
      "tensor(0.3036)\n",
      "tensor(0.3027)\n",
      "tensor(0.3018)\n",
      "tensor(0.3008)\n",
      "tensor(0.2999)\n",
      "tensor(0.2990)\n",
      "tensor(0.2981)\n",
      "tensor(0.2972)\n",
      "tensor(0.2963)\n",
      "tensor(0.2954)\n",
      "tensor(0.2945)\n",
      "tensor(0.2936)\n",
      "tensor(0.2927)\n",
      "tensor(0.2918)\n",
      "tensor(0.2910)\n",
      "tensor(0.2901)\n",
      "tensor(0.2892)\n",
      "tensor(0.2884)\n",
      "tensor(0.2875)\n",
      "tensor(0.2866)\n",
      "tensor(0.2858)\n",
      "tensor(0.2849)\n",
      "tensor(0.2841)\n",
      "tensor(0.2832)\n",
      "tensor(0.2824)\n",
      "tensor(0.2816)\n",
      "tensor(0.2807)\n",
      "tensor(0.2799)\n",
      "tensor(0.2791)\n",
      "tensor(0.2783)\n",
      "tensor(0.2774)\n",
      "tensor(0.2766)\n",
      "tensor(0.2758)\n",
      "tensor(0.2750)\n",
      "tensor(0.2742)\n",
      "tensor(0.2734)\n",
      "tensor(0.2726)\n",
      "tensor(0.2718)\n",
      "tensor(0.2710)\n",
      "tensor(0.2702)\n",
      "tensor(0.2695)\n",
      "tensor(0.2687)\n",
      "tensor(0.2679)\n",
      "tensor(0.2671)\n",
      "tensor(0.2664)\n",
      "tensor(0.2656)\n",
      "tensor(0.2648)\n",
      "tensor(0.2641)\n",
      "tensor(0.2633)\n",
      "tensor(0.2626)\n",
      "tensor(0.2618)\n",
      "tensor(0.2611)\n",
      "tensor(0.2603)\n",
      "tensor(0.2596)\n",
      "tensor(0.2588)\n",
      "tensor(0.2581)\n",
      "tensor(0.2574)\n",
      "tensor(0.2566)\n",
      "tensor(0.2559)\n",
      "tensor(0.2552)\n",
      "tensor(0.2545)\n",
      "tensor(0.2537)\n",
      "tensor(0.2530)\n",
      "tensor(0.2523)\n",
      "tensor(0.2516)\n",
      "tensor(0.2509)\n",
      "tensor(0.2502)\n",
      "tensor(0.2495)\n",
      "tensor(0.2488)\n",
      "tensor(0.2481)\n",
      "tensor(0.2474)\n",
      "tensor(0.2467)\n",
      "tensor(0.2460)\n",
      "tensor(0.2453)\n",
      "tensor(0.2447)\n",
      "tensor(0.2440)\n",
      "tensor(0.2433)\n",
      "tensor(0.2426)\n",
      "tensor(0.2420)\n",
      "tensor(0.2413)\n",
      "tensor(0.2406)\n",
      "tensor(0.2400)\n",
      "tensor(0.2393)\n",
      "tensor(0.2387)\n",
      "tensor(0.2380)\n",
      "tensor(0.2373)\n",
      "tensor(0.2367)\n",
      "tensor(0.2360)\n",
      "tensor(0.2354)\n",
      "tensor(0.2348)\n",
      "tensor(0.2341)\n",
      "tensor(0.2335)\n",
      "tensor(0.2328)\n",
      "tensor(0.2322)\n",
      "tensor(0.2316)\n",
      "tensor(0.2309)\n",
      "tensor(0.2303)\n",
      "tensor(0.2297)\n",
      "tensor(0.2291)\n",
      "tensor(0.2285)\n",
      "tensor(0.2278)\n",
      "tensor(0.2272)\n",
      "tensor(0.2266)\n",
      "tensor(0.2260)\n",
      "tensor(0.2254)\n",
      "tensor(0.2248)\n",
      "tensor(0.2242)\n",
      "tensor(0.2236)\n",
      "tensor(0.2230)\n",
      "tensor(0.2224)\n",
      "tensor(0.2218)\n",
      "tensor(0.2212)\n",
      "tensor(0.2206)\n",
      "tensor(0.2200)\n",
      "tensor(0.2195)\n",
      "tensor(0.2189)\n",
      "tensor(0.2183)\n",
      "tensor(0.2177)\n",
      "tensor(0.2171)\n",
      "tensor(0.2166)\n",
      "tensor(0.2160)\n",
      "tensor(0.2154)\n",
      "tensor(0.2149)\n",
      "tensor(0.2143)\n",
      "tensor(0.2137)\n",
      "tensor(0.2132)\n",
      "tensor(0.2126)\n",
      "tensor(0.2120)\n",
      "tensor(0.2115)\n",
      "tensor(0.2109)\n",
      "tensor(0.2104)\n",
      "tensor(0.2098)\n",
      "tensor(0.2093)\n",
      "tensor(0.2087)\n",
      "tensor(0.2082)\n",
      "tensor(0.2077)\n",
      "tensor(0.2071)\n",
      "tensor(0.2066)\n",
      "tensor(0.2061)\n",
      "tensor(0.2055)\n",
      "tensor(0.2050)\n",
      "tensor(0.2045)\n",
      "tensor(0.2039)\n",
      "tensor(0.2034)\n",
      "tensor(0.2029)\n",
      "tensor(0.2024)\n",
      "tensor(0.2018)\n",
      "tensor(0.2013)\n",
      "tensor(0.2008)\n",
      "tensor(0.2003)\n",
      "tensor(0.1998)\n",
      "tensor(0.1993)\n",
      "tensor(0.1988)\n",
      "tensor(0.1983)\n",
      "tensor(0.1978)\n",
      "tensor(0.1973)\n",
      "tensor(0.1968)\n",
      "tensor(0.1963)\n",
      "tensor(0.1958)\n",
      "tensor(0.1953)\n",
      "tensor(0.1948)\n",
      "tensor(0.1943)\n",
      "tensor(0.1938)\n",
      "tensor(0.1933)\n",
      "tensor(0.1928)\n",
      "tensor(0.1923)\n",
      "tensor(0.1918)\n",
      "tensor(0.1914)\n",
      "tensor(0.1909)\n",
      "tensor(0.1904)\n",
      "tensor(0.1899)\n",
      "tensor(0.1895)\n",
      "tensor(0.1890)\n",
      "tensor(0.1885)\n",
      "tensor(0.1880)\n",
      "tensor(0.1876)\n",
      "tensor(0.1871)\n",
      "tensor(0.1866)\n",
      "tensor(0.1862)\n",
      "tensor(0.1857)\n",
      "tensor(0.1853)\n",
      "tensor(0.1848)\n",
      "tensor(0.1843)\n",
      "tensor(0.1839)\n",
      "tensor(0.1834)\n",
      "tensor(0.1830)\n",
      "tensor(0.1825)\n",
      "tensor(0.1821)\n",
      "tensor(0.1816)\n",
      "tensor(0.1812)\n",
      "tensor(0.1808)\n",
      "tensor(0.1803)\n",
      "tensor(0.1799)\n",
      "tensor(0.1794)\n",
      "tensor(0.1790)\n",
      "tensor(0.1786)\n",
      "tensor(0.1781)\n",
      "tensor(0.1777)\n",
      "tensor(0.1773)\n",
      "tensor(0.1768)\n",
      "tensor(0.1764)\n",
      "tensor(0.1760)\n",
      "tensor(0.1756)\n",
      "tensor(0.1751)\n",
      "tensor(0.1747)\n",
      "tensor(0.1743)\n",
      "tensor(0.1739)\n",
      "tensor(0.1735)\n",
      "tensor(0.1730)\n",
      "tensor(0.1726)\n",
      "tensor(0.1722)\n",
      "tensor(0.1718)\n",
      "tensor(0.1714)\n",
      "tensor(0.1710)\n",
      "tensor(0.1706)\n",
      "tensor(0.1702)\n",
      "tensor(0.1698)\n",
      "tensor(0.1694)\n",
      "tensor(0.1689)\n",
      "tensor(0.1685)\n",
      "tensor(0.1681)\n",
      "tensor(0.1678)\n",
      "tensor(0.1674)\n",
      "tensor(0.1670)\n",
      "tensor(0.1666)\n",
      "tensor(0.1662)\n",
      "tensor(0.1658)\n",
      "tensor(0.1654)\n",
      "tensor(0.1650)\n",
      "tensor(0.1646)\n",
      "tensor(0.1642)\n",
      "tensor(0.1638)\n",
      "tensor(0.1635)\n",
      "tensor(0.1631)\n",
      "tensor(0.1627)\n",
      "tensor(0.1623)\n",
      "tensor(0.1619)\n",
      "tensor(0.1616)\n",
      "tensor(0.1612)\n",
      "tensor(0.1608)\n",
      "tensor(0.1604)\n",
      "tensor(0.1601)\n",
      "tensor(0.1597)\n",
      "tensor(0.1593)\n",
      "tensor(0.1590)\n",
      "tensor(0.1586)\n",
      "tensor(0.1582)\n",
      "tensor(0.1579)\n",
      "tensor(0.1575)\n",
      "tensor(0.1571)\n",
      "tensor(0.1568)\n",
      "tensor(0.1564)\n",
      "tensor(0.1561)\n",
      "tensor(0.1557)\n",
      "tensor(0.1554)\n",
      "tensor(0.1550)\n",
      "tensor(0.1546)\n",
      "tensor(0.1543)\n",
      "tensor(0.1539)\n",
      "tensor(0.1536)\n",
      "tensor(0.1532)\n",
      "tensor(0.1529)\n",
      "tensor(0.1526)\n",
      "tensor(0.1522)\n",
      "tensor(0.1519)\n",
      "tensor(0.1515)\n",
      "tensor(0.1512)\n",
      "tensor(0.1508)\n",
      "tensor(0.1505)\n",
      "tensor(0.1502)\n",
      "tensor(0.1498)\n",
      "tensor(0.1495)\n",
      "tensor(0.1492)\n",
      "tensor(0.1488)\n",
      "tensor(0.1485)\n",
      "tensor(0.1482)\n",
      "tensor(0.1478)\n",
      "tensor(0.1475)\n",
      "tensor(0.1472)\n",
      "tensor(0.1468)\n",
      "tensor(0.1465)\n",
      "tensor(0.1462)\n",
      "tensor(0.1459)\n",
      "tensor(0.1456)\n",
      "tensor(0.1452)\n",
      "tensor(0.1449)\n",
      "tensor(0.1446)\n",
      "tensor(0.1443)\n",
      "tensor(0.1440)\n",
      "tensor(0.1436)\n",
      "tensor(0.1433)\n",
      "tensor(0.1430)\n",
      "tensor(0.1427)\n",
      "tensor(0.1424)\n",
      "tensor(0.1421)\n",
      "tensor(0.1418)\n",
      "tensor(0.1415)\n",
      "tensor(0.1411)\n",
      "tensor(0.1408)\n",
      "tensor(0.1405)\n",
      "tensor(0.1402)\n",
      "tensor(0.1399)\n",
      "tensor(0.1396)\n",
      "tensor(0.1393)\n",
      "tensor(0.1390)\n",
      "tensor(0.1387)\n",
      "tensor(0.1384)\n",
      "tensor(0.1381)\n",
      "tensor(0.1378)\n",
      "tensor(0.1375)\n",
      "tensor(0.1372)\n",
      "tensor(0.1369)\n",
      "tensor(0.1366)\n",
      "tensor(0.1364)\n",
      "tensor(0.1361)\n",
      "tensor(0.1358)\n",
      "tensor(0.1355)\n",
      "tensor(0.1352)\n",
      "tensor(0.1349)\n",
      "tensor(0.1346)\n",
      "tensor(0.1343)\n",
      "tensor(0.1340)\n",
      "tensor(0.1338)\n",
      "tensor(0.1335)\n",
      "tensor(0.1332)\n",
      "tensor(0.1329)\n",
      "tensor(0.1326)\n",
      "tensor(0.1324)\n",
      "tensor(0.1321)\n",
      "tensor(0.1318)\n",
      "tensor(0.1315)\n",
      "tensor(0.1312)\n",
      "tensor(0.1310)\n",
      "tensor(0.1307)\n",
      "tensor(0.1304)\n",
      "tensor(0.1302)\n",
      "tensor(0.1299)\n",
      "tensor(0.1296)\n",
      "tensor(0.1293)\n",
      "tensor(0.1291)\n",
      "tensor(0.1288)\n",
      "tensor(0.1285)\n",
      "tensor(0.1283)\n",
      "tensor(0.1280)\n",
      "tensor(0.1277)\n",
      "tensor(0.1275)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1272)\n",
      "tensor(0.1270)\n",
      "tensor(0.1267)\n",
      "tensor(0.1264)\n",
      "tensor(0.1262)\n",
      "tensor(0.1259)\n",
      "tensor(0.1257)\n",
      "tensor(0.1254)\n",
      "tensor(0.1251)\n",
      "tensor(0.1249)\n",
      "tensor(0.1246)\n",
      "tensor(0.1244)\n",
      "tensor(0.1241)\n",
      "tensor(0.1239)\n",
      "tensor(0.1236)\n",
      "tensor(0.1234)\n",
      "tensor(0.1231)\n",
      "tensor(0.1229)\n",
      "tensor(0.1226)\n",
      "tensor(0.1224)\n",
      "tensor(0.1221)\n",
      "tensor(0.1219)\n",
      "tensor(0.1216)\n",
      "tensor(0.1214)\n",
      "tensor(0.1211)\n",
      "tensor(0.1209)\n",
      "tensor(0.1207)\n",
      "tensor(0.1204)\n",
      "tensor(0.1202)\n",
      "tensor(0.1199)\n",
      "tensor(0.1197)\n",
      "tensor(0.1195)\n",
      "tensor(0.1192)\n",
      "tensor(0.1190)\n",
      "tensor(0.1188)\n",
      "tensor(0.1185)\n",
      "tensor(0.1183)\n",
      "tensor(0.1180)\n",
      "tensor(0.1178)\n",
      "tensor(0.1176)\n",
      "tensor(0.1173)\n",
      "tensor(0.1171)\n",
      "tensor(0.1169)\n",
      "tensor(0.1167)\n",
      "tensor(0.1164)\n",
      "tensor(0.1162)\n",
      "tensor(0.1160)\n",
      "tensor(0.1157)\n",
      "tensor(0.1155)\n",
      "tensor(0.1153)\n",
      "tensor(0.1151)\n",
      "tensor(0.1148)\n",
      "tensor(0.1146)\n",
      "tensor(0.1144)\n",
      "tensor(0.1142)\n",
      "tensor(0.1140)\n",
      "tensor(0.1137)\n",
      "tensor(0.1135)\n",
      "tensor(0.1133)\n",
      "tensor(0.1131)\n",
      "tensor(0.1129)\n",
      "tensor(0.1126)\n",
      "tensor(0.1124)\n",
      "tensor(0.1122)\n",
      "tensor(0.1120)\n",
      "tensor(0.1118)\n",
      "tensor(0.1116)\n",
      "tensor(0.1113)\n",
      "tensor(0.1111)\n",
      "tensor(0.1109)\n",
      "tensor(0.1107)\n",
      "tensor(0.1105)\n",
      "tensor(0.1103)\n",
      "tensor(0.1101)\n",
      "tensor(0.1099)\n",
      "tensor(0.1097)\n",
      "tensor(0.1095)\n",
      "tensor(0.1092)\n",
      "tensor(0.1090)\n",
      "tensor(0.1088)\n",
      "tensor(0.1086)\n",
      "tensor(0.1084)\n",
      "tensor(0.1082)\n",
      "tensor(0.1080)\n",
      "tensor(0.1078)\n",
      "tensor(0.1076)\n",
      "tensor(0.1074)\n",
      "tensor(0.1072)\n",
      "tensor(0.1070)\n",
      "tensor(0.1068)\n",
      "tensor(0.1066)\n",
      "tensor(0.1064)\n",
      "tensor(0.1062)\n",
      "tensor(0.1060)\n",
      "tensor(0.1058)\n",
      "tensor(0.1056)\n",
      "tensor(0.1054)\n",
      "tensor(0.1052)\n",
      "tensor(0.1050)\n",
      "tensor(0.1048)\n",
      "tensor(0.1046)\n",
      "tensor(0.1044)\n",
      "tensor(0.1043)\n",
      "tensor(0.1041)\n",
      "tensor(0.1039)\n",
      "tensor(0.1037)\n",
      "tensor(0.1035)\n",
      "tensor(0.1033)\n",
      "tensor(0.1031)\n",
      "tensor(0.1029)\n",
      "tensor(0.1027)\n",
      "tensor(0.1025)\n",
      "tensor(0.1024)\n",
      "tensor(0.1022)\n",
      "tensor(0.1020)\n",
      "tensor(0.1018)\n",
      "tensor(0.1016)\n",
      "tensor(0.1014)\n",
      "tensor(0.1012)\n",
      "tensor(0.1011)\n",
      "tensor(0.1009)\n",
      "tensor(0.1007)\n",
      "tensor(0.1005)\n",
      "tensor(0.1003)\n",
      "tensor(0.1002)\n",
      "tensor(1.00000e-02 *\n",
      "       9.9978)\n",
      "tensor(1.00000e-02 *\n",
      "       9.9798)\n",
      "tensor(1.00000e-02 *\n",
      "       9.9619)\n",
      "tensor(1.00000e-02 *\n",
      "       9.9440)\n",
      "tensor(1.00000e-02 *\n",
      "       9.9262)\n",
      "tensor(1.00000e-02 *\n",
      "       9.9085)\n",
      "tensor(1.00000e-02 *\n",
      "       9.8908)\n",
      "tensor(1.00000e-02 *\n",
      "       9.8731)\n",
      "tensor(1.00000e-02 *\n",
      "       9.8555)\n",
      "tensor(1.00000e-02 *\n",
      "       9.8380)\n",
      "tensor(1.00000e-02 *\n",
      "       9.8205)\n",
      "tensor(1.00000e-02 *\n",
      "       9.8030)\n",
      "tensor(1.00000e-02 *\n",
      "       9.7856)\n",
      "tensor(1.00000e-02 *\n",
      "       9.7683)\n",
      "tensor(1.00000e-02 *\n",
      "       9.7510)\n",
      "tensor(1.00000e-02 *\n",
      "       9.7337)\n",
      "tensor(1.00000e-02 *\n",
      "       9.7165)\n",
      "tensor(1.00000e-02 *\n",
      "       9.6994)\n",
      "tensor(1.00000e-02 *\n",
      "       9.6823)\n",
      "tensor(1.00000e-02 *\n",
      "       9.6652)\n",
      "tensor(1.00000e-02 *\n",
      "       9.6482)\n",
      "tensor(1.00000e-02 *\n",
      "       9.6312)\n",
      "tensor(1.00000e-02 *\n",
      "       9.6143)\n",
      "tensor(1.00000e-02 *\n",
      "       9.5975)\n",
      "tensor(1.00000e-02 *\n",
      "       9.5807)\n",
      "tensor(1.00000e-02 *\n",
      "       9.5639)\n",
      "tensor(1.00000e-02 *\n",
      "       9.5472)\n",
      "tensor(1.00000e-02 *\n",
      "       9.5305)\n",
      "tensor(1.00000e-02 *\n",
      "       9.5139)\n",
      "tensor(1.00000e-02 *\n",
      "       9.4973)\n",
      "tensor(1.00000e-02 *\n",
      "       9.4807)\n",
      "tensor(1.00000e-02 *\n",
      "       9.4642)\n",
      "tensor(1.00000e-02 *\n",
      "       9.4478)\n",
      "tensor(1.00000e-02 *\n",
      "       9.4314)\n",
      "tensor(1.00000e-02 *\n",
      "       9.4151)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3988)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3825)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3663)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3501)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3340)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3179)\n",
      "tensor(1.00000e-02 *\n",
      "       9.3019)\n",
      "tensor(1.00000e-02 *\n",
      "       9.2859)\n",
      "tensor(1.00000e-02 *\n",
      "       9.2700)\n",
      "tensor(1.00000e-02 *\n",
      "       9.2541)\n",
      "tensor(1.00000e-02 *\n",
      "       9.2382)\n",
      "tensor(1.00000e-02 *\n",
      "       9.2224)\n",
      "tensor(1.00000e-02 *\n",
      "       9.2066)\n",
      "tensor(1.00000e-02 *\n",
      "       9.1909)\n",
      "tensor(1.00000e-02 *\n",
      "       9.1752)\n",
      "tensor(1.00000e-02 *\n",
      "       9.1596)\n",
      "tensor(1.00000e-02 *\n",
      "       9.1440)\n",
      "tensor(1.00000e-02 *\n",
      "       9.1284)\n",
      "tensor(1.00000e-02 *\n",
      "       9.1129)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0975)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0820)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0667)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0513)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0360)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0208)\n",
      "tensor(1.00000e-02 *\n",
      "       9.0056)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9904)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9753)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9602)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9451)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9301)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9152)\n",
      "tensor(1.00000e-02 *\n",
      "       8.9002)\n",
      "tensor(1.00000e-02 *\n",
      "       8.8854)\n",
      "tensor(1.00000e-02 *\n",
      "       8.8705)\n",
      "tensor(1.00000e-02 *\n",
      "       8.8557)\n",
      "tensor(1.00000e-02 *\n",
      "       8.8410)\n",
      "tensor(1.00000e-02 *\n",
      "       8.8262)\n",
      "tensor(1.00000e-02 *\n",
      "       8.8116)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7969)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7823)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7678)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7532)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7388)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7243)\n",
      "tensor(1.00000e-02 *\n",
      "       8.7099)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6955)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6812)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6669)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6527)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6384)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6243)\n",
      "tensor(1.00000e-02 *\n",
      "       8.6101)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5960)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5820)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5680)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5540)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5400)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5261)\n",
      "tensor(1.00000e-02 *\n",
      "       8.5122)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4984)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4846)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4708)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4571)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4434)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4298)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4162)\n",
      "tensor(1.00000e-02 *\n",
      "       8.4026)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3890)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3755)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3620)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3486)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3352)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3218)\n",
      "tensor(1.00000e-02 *\n",
      "       8.3085)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2952)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2819)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2687)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2555)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2424)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2292)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2162)\n",
      "tensor(1.00000e-02 *\n",
      "       8.2031)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1901)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1771)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1641)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1512)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1383)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1255)\n",
      "tensor(1.00000e-02 *\n",
      "       8.1127)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0999)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0871)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0744)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0617)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0491)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0365)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0239)\n",
      "tensor(1.00000e-02 *\n",
      "       8.0113)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9988)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9863)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9738)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9614)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9490)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9367)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9243)\n",
      "tensor(1.00000e-02 *\n",
      "       7.9120)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8998)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8875)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8753)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8632)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8510)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8389)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8268)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8148)\n",
      "tensor(1.00000e-02 *\n",
      "       7.8028)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7908)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7788)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7669)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7550)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7431)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7313)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7195)\n",
      "tensor(1.00000e-02 *\n",
      "       7.7077)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6960)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6843)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6726)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6609)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6493)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6377)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6261)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6146)\n",
      "tensor(1.00000e-02 *\n",
      "       7.6031)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5916)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5801)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5687)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5573)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5460)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5346)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5233)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5120)\n",
      "tensor(1.00000e-02 *\n",
      "       7.5008)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4895)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4783)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4672)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4560)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       7.4449)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4338)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4228)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4117)\n",
      "tensor(1.00000e-02 *\n",
      "       7.4007)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3898)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3788)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3679)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3570)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3461)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3353)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3245)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3137)\n",
      "tensor(1.00000e-02 *\n",
      "       7.3029)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2922)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2815)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2708)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2601)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2495)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2389)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2283)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2178)\n",
      "tensor(1.00000e-02 *\n",
      "       7.2072)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1967)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1863)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1758)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1654)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1550)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1446)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1343)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1239)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1136)\n",
      "tensor(1.00000e-02 *\n",
      "       7.1034)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0931)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0829)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0727)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0625)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0524)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0422)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0321)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0221)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0120)\n",
      "tensor(1.00000e-02 *\n",
      "       7.0020)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9920)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9820)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9720)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9621)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9522)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9423)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9324)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9226)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9128)\n",
      "tensor(1.00000e-02 *\n",
      "       6.9030)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8932)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8835)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8738)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8641)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8544)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8447)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8351)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8255)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8159)\n",
      "tensor(1.00000e-02 *\n",
      "       6.8064)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7968)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7873)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7778)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7683)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7589)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7495)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7401)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7307)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7213)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7120)\n",
      "tensor(1.00000e-02 *\n",
      "       6.7027)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6934)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6841)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6748)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6656)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6564)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6472)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6380)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6289)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6198)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6107)\n",
      "tensor(1.00000e-02 *\n",
      "       6.6016)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5925)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5835)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5745)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5655)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5565)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5476)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5386)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5297)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5208)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5119)\n",
      "tensor(1.00000e-02 *\n",
      "       6.5031)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4943)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4854)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4767)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4679)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4591)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4504)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4417)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4330)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4243)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4157)\n",
      "tensor(1.00000e-02 *\n",
      "       6.4070)\n"
     ]
    }
   ],
   "source": [
    "# In a training loop, we should perform many GD iterations.\n",
    "n_iter = 1000\n",
    "for i in range(n_iter):\n",
    "    optimizer.zero_grad() # equivalent to net.zero_grad()\n",
    "    output = net(x)\n",
    "    loss = criterion(output,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.9596, -1.4903, -1.5973,  0.2635, -1.4419, -1.2406, -1.1636,\n",
      "         -1.2870, -1.6135,  4.0923],\n",
      "        [ 0.4499, -1.1107, -1.1447,  6.3220, -1.2572, -1.0299, -1.3596,\n",
      "         -1.2432, -1.0662,  3.7187],\n",
      "        [ 2.1048, -1.2875, -1.2229,  3.2101, -1.2439, -1.1290, -1.3430,\n",
      "         -1.3652, -1.3110,  5.8874]])\n",
      "tensor([ 0,  3,  9])\n"
     ]
    }
   ],
   "source": [
    "output = net(x)\n",
    "print(output)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to train a network ! For a complete training check the pytorch_example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['0.weight', '0.bias', '2.weight', '2.bias'])\n"
     ]
    }
   ],
   "source": [
    "# get dictionary of keys to weights using `state_dict`\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28*28,256),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(256,10))\n",
    "print(net.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dictionary\n",
    "torch.save(net.state_dict(),'test.t7')\n",
    "# load a dictionary\n",
    "net.load_state_dict(torch.load('test.t7'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common issues to look out for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'mat2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-4d1c8f2c4847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dl/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/dl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.LongTensor but found type torch.FloatTensor for argument #2 'mat2'"
     ]
    }
   ],
   "source": [
    "net = nn.Linear(4,2)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.float()\n",
    "x = torch.tensor([1.,2.,3.,4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6.,  6.],\n",
      "        [ 6.,  6.]])\n",
      "tensor([[ 12.,  12.],\n",
      "        [ 12.,  12.]])\n"
     ]
    }
   ],
   "source": [
    "x = 2* torch.ones(2,2)\n",
    "y = 3* torch.ones(2,2)\n",
    "print(x * y)\n",
    "print(x.matmul(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.],\n",
      "        [ 1.,  2.,  3.,  4.,  5.]])\n",
      "tensor([[ 1.,  1.,  1.,  1.,  1.],\n",
      "        [ 2.,  2.,  2.,  2.,  2.],\n",
      "        [ 3.,  3.,  3.,  3.,  3.],\n",
      "        [ 4.,  4.,  4.,  4.,  4.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-8799a16e988f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,5)\n",
    "y = torch.arange(5)\n",
    "print(x+y)\n",
    "y = torch.arange(4).view(-1,1)\n",
    "print(x+y)\n",
    "y = torch.arange(4)\n",
    "print(x+y) # exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6]])\n",
      "tensor([[ 1,  4],\n",
      "        [ 2,  5],\n",
      "        [ 3,  6]])\n",
      "tensor([[ 1,  2],\n",
      "        [ 3,  4],\n",
      "        [ 5,  6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(x)\n",
    "print(x.t())\n",
    "print(x.view(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,2048),nn.ReLU(),\n",
    "                   nn.Linear(2048,120))\n",
    "x = torch.ones(256,2048)\n",
    "y = torch.zeros(256).long()\n",
    "net.cuda()\n",
    "x.cuda()\n",
    "crit=nn.CrossEntropyLoss()\n",
    "out = net(x)\n",
    "loss = crit(out,y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "    \n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,n_hidden_layers):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.final_layer = nn.Linear(128,10)\n",
    "        self.act = nn.ReLU()\n",
    "        self.hidden = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            self.hidden.append(nn.Linear(128,128))\n",
    "        self.hidden = nn.ModuleList(self.hidden)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        h = x\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            h = self.hidden[i](h)\n",
    "            h = self.act(h)\n",
    "        out = self.final_layer(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
